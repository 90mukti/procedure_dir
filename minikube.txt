
==> Audit <==
|---------|-----------------------|----------|-------|---------|---------------------|---------------------|
| Command |         Args          | Profile  | User  | Version |     Start Time      |      End Time       |
|---------|-----------------------|----------|-------|---------|---------------------|---------------------|
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 12:55 EDT |                     |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 12:58 EDT |                     |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 12:59 EDT | 08 May 24 12:59 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 12:59 EDT | 08 May 24 12:59 EDT |
| stop    |                       | minikube | kauls | v1.33.0 | 08 May 24 13:05 EDT | 08 May 24 13:05 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 13:05 EDT | 08 May 24 13:05 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 16:35 EDT | 08 May 24 16:36 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 16:36 EDT | 08 May 24 16:36 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 16:42 EDT | 08 May 24 16:42 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 16:44 EDT | 08 May 24 16:44 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 16:49 EDT | 08 May 24 16:49 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 16:49 EDT | 08 May 24 16:49 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 16:53 EDT | 08 May 24 16:53 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 16:53 EDT | 08 May 24 16:54 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 16:55 EDT | 08 May 24 16:55 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 16:56 EDT | 08 May 24 16:57 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 17:00 EDT | 08 May 24 17:00 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 17:01 EDT | 08 May 24 17:02 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 17:02 EDT | 08 May 24 17:02 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 17:02 EDT | 08 May 24 17:02 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 17:02 EDT | 08 May 24 17:03 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 17:03 EDT | 08 May 24 17:03 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 17:04 EDT | 08 May 24 17:04 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 17:09 EDT | 08 May 24 17:09 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 17:10 EDT | 08 May 24 17:10 EDT |
| delete  |                       | minikube | kauls | v1.33.0 | 08 May 24 17:51 EDT | 08 May 24 17:51 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 08 May 24 17:52 EDT | 08 May 24 17:52 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 09 May 24 10:05 EDT | 09 May 24 10:06 EDT |
| stop    |                       | minikube | kauls | v1.33.0 | 09 May 24 10:33 EDT | 09 May 24 10:33 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 09 May 24 10:33 EDT | 09 May 24 10:34 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 09 May 24 10:45 EDT | 09 May 24 10:46 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 09 May 24 10:55 EDT | 09 May 24 10:56 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 09 May 24 16:33 EDT | 09 May 24 16:33 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 09 May 24 19:16 EDT |                     |
| start   |                       | minikube | kauls | v1.33.0 | 10 May 24 16:48 EDT |                     |
| start   |                       | minikube | kauls | v1.33.0 | 10 May 24 17:16 EDT |                     |
| start   | --driver=docker       | minikube | kauls | v1.33.0 | 10 May 24 17:16 EDT |                     |
| start   | --driver=docker       | minikube | kauls | v1.33.0 | 10 May 24 17:18 EDT |                     |
| start   |                       | minikube | kauls | v1.33.0 | 10 May 24 17:18 EDT |                     |
| start   |                       | minikube | kauls | v1.33.0 | 10 May 24 17:19 EDT | 10 May 24 17:20 EDT |
| stop    |                       | minikube | kauls | v1.33.0 | 10 May 24 17:21 EDT | 10 May 24 17:21 EDT |
| start   | --driver=docker       | minikube | kauls | v1.33.0 | 10 May 24 17:21 EDT | 10 May 24 17:22 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 11 May 24 12:58 EDT | 11 May 24 12:59 EDT |
| stop    |                       | minikube | kauls | v1.33.0 | 11 May 24 13:02 EDT | 11 May 24 13:02 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 11 May 24 13:02 EDT | 11 May 24 13:03 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 11 May 24 13:20 EDT | 11 May 24 13:21 EDT |
| stop    |                       | minikube | kauls | v1.33.0 | 11 May 24 13:21 EDT | 11 May 24 13:21 EDT |
| start   | --driver=docker       | minikube | kauls | v1.33.0 | 11 May 24 13:21 EDT | 11 May 24 13:22 EDT |
| start   | --driver=263ea3dd0c94 | minikube | kauls | v1.33.0 | 11 May 24 16:37 EDT |                     |
| delete  | thor                  | minikube | kauls | v1.33.0 | 11 May 24 16:38 EDT |                     |
| delete  |                       | minikube | kauls | v1.33.0 | 11 May 24 16:38 EDT | 11 May 24 16:38 EDT |
| start   | --driver=263ea3dd0c94 | minikube | kauls | v1.33.0 | 11 May 24 16:38 EDT |                     |
| start   | --driver=docker       | minikube | kauls | v1.33.0 | 11 May 24 16:39 EDT | 11 May 24 16:39 EDT |
| start   | --driver=docker       | minikube | kauls | v1.33.0 | 12 May 24 16:02 EDT | 12 May 24 16:02 EDT |
| delete  | thor                  | minikube | kauls | v1.33.0 | 12 May 24 16:08 EDT |                     |
| delete  | deployment            | minikube | kauls | v1.33.0 | 12 May 24 16:08 EDT |                     |
| stop    |                       | minikube | kauls | v1.33.0 | 12 May 24 16:08 EDT | 12 May 24 16:09 EDT |
| start   |                       | minikube | kauls | v1.33.0 | 12 May 24 16:09 EDT | 12 May 24 16:09 EDT |
| stop    |                       | minikube | kauls | v1.33.0 | 12 May 24 16:10 EDT | 12 May 24 16:10 EDT |
| start   | --driver=docker       | minikube | kauls | v1.33.0 | 13 May 24 11:28 EDT | 13 May 24 11:29 EDT |
|---------|-----------------------|----------|-------|---------|---------------------|---------------------|


==> Last Start <==
Log file created at: 2024/05/13 11:28:49
Running on machine: XPS15-SK
Binary: Built with gc go1.22.1 for linux/amd64
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0513 11:28:49.988270    1464 out.go:291] Setting OutFile to fd 1 ...
I0513 11:28:49.989354    1464 out.go:343] isatty.IsTerminal(1) = true
I0513 11:28:49.989358    1464 out.go:304] Setting ErrFile to fd 2...
I0513 11:28:49.989363    1464 out.go:343] isatty.IsTerminal(2) = true
I0513 11:28:49.989467    1464 root.go:338] Updating PATH: /home/kauls/.minikube/bin
I0513 11:28:49.991010    1464 out.go:298] Setting JSON to false
I0513 11:28:49.991730    1464 start.go:129] hostinfo: {"hostname":"XPS15-SK","uptime":229,"bootTime":1715613901,"procs":49,"os":"linux","platform":"ubuntu","platformFamily":"debian","platformVersion":"22.04","kernelVersion":"5.15.146.1-microsoft-standard-WSL2","kernelArch":"x86_64","virtualizationSystem":"","virtualizationRole":"guest","hostId":"57d2e4d5-9803-a5c2-c1cc-14726501e47d"}
I0513 11:28:49.991762    1464 start.go:139] virtualization:  guest
I0513 11:28:49.995310    1464 out.go:177] 😄  minikube v1.33.0 on Ubuntu 22.04 (amd64)
I0513 11:28:50.002733    1464 notify.go:220] Checking for updates...
I0513 11:28:50.003576    1464 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0513 11:28:50.004704    1464 driver.go:392] Setting default libvirt URI to qemu:///system
I0513 11:28:50.020370    1464 docker.go:122] docker version: linux-24.0.5:
I0513 11:28:50.020444    1464 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0513 11:28:50.055743    1464 info.go:266] docker info: {ID:fd818e5a-2233-424b-b5fb-578275ec18d3 Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:35 SystemTime:2024-05-13 11:28:50.049144033 -0400 EDT LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16623325184 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:XPS15-SK Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0]] Warnings:<nil>}}
I0513 11:28:50.055798    1464 docker.go:295] overlay module found
I0513 11:28:50.058885    1464 out.go:177] ✨  Using the docker driver based on existing profile
I0513 11:28:50.063698    1464 start.go:297] selected driver: docker
I0513 11:28:50.063714    1464 start.go:901] validating driver "docker" against &{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/kauls:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0513 11:28:50.063767    1464 start.go:912] status for docker: {Installed:true Healthy:true Running:false NeedsImprovement:false Error:<nil> Reason: Fix: Doc: Version:}
I0513 11:28:50.063819    1464 cli_runner.go:164] Run: docker system info --format "{{json .}}"
I0513 11:28:50.102740    1464 info.go:266] docker info: {ID:fd818e5a-2233-424b-b5fb-578275ec18d3 Containers:5 ContainersRunning:0 ContainersPaused:0 ContainersStopped:5 Images:4 Driver:overlay2 DriverStatus:[[Backing Filesystem extfs] [Supports d_type true] [Using metacopy false] [Native Overlay Diff true] [userxattr false]] SystemStatus:<nil> Plugins:{Volume:[local] Network:[bridge host ipvlan macvlan null overlay] Authorization:<nil> Log:[awslogs fluentd gcplogs gelf journald json-file local logentries splunk syslog]} MemoryLimit:true SwapLimit:true KernelMemory:false KernelMemoryTCP:true CPUCfsPeriod:true CPUCfsQuota:true CPUShares:true CPUSet:true PidsLimit:true IPv4Forwarding:true BridgeNfIptables:true BridgeNfIP6Tables:true Debug:false NFd:24 OomKillDisable:true NGoroutines:35 SystemTime:2024-05-13 11:28:50.09716672 -0400 EDT LoggingDriver:json-file CgroupDriver:cgroupfs NEventsListener:0 KernelVersion:5.15.146.1-microsoft-standard-WSL2 OperatingSystem:Ubuntu 22.04.4 LTS OSType:linux Architecture:x86_64 IndexServerAddress:https://index.docker.io/v1/ RegistryConfig:{AllowNondistributableArtifactsCIDRs:[] AllowNondistributableArtifactsHostnames:[] InsecureRegistryCIDRs:[127.0.0.0/8] IndexConfigs:{DockerIo:{Name:docker.io Mirrors:[] Secure:true Official:true}} Mirrors:[]} NCPU:12 MemTotal:16623325184 GenericResources:<nil> DockerRootDir:/var/lib/docker HTTPProxy: HTTPSProxy: NoProxy: Name:XPS15-SK Labels:[] ExperimentalBuild:false ServerVersion:24.0.5 ClusterStore: ClusterAdvertise: Runtimes:{Runc:{Path:runc}} DefaultRuntime:runc Swarm:{NodeID: NodeAddr: LocalNodeState:inactive ControlAvailable:false Error: RemoteManagers:<nil>} LiveRestoreEnabled:false Isolation: InitBinary:docker-init ContainerdCommit:{ID: Expected:} RuncCommit:{ID: Expected:} InitCommit:{ID: Expected:} SecurityOptions:[name=seccomp,profile=builtin] ProductLicense: Warnings:[WARNING: No blkio throttle.read_bps_device support WARNING: No blkio throttle.write_bps_device support WARNING: No blkio throttle.read_iops_device support WARNING: No blkio throttle.write_iops_device support] ServerErrors:[] ClientInfo:{Debug:false Plugins:[map[Name:buildx Path:/usr/libexec/docker/cli-plugins/docker-buildx SchemaVersion:0.1.0 ShortDescription:Docker Buildx Vendor:Docker Inc. Version:v0.14.0]] Warnings:<nil>}}
I0513 11:28:50.103228    1464 cni.go:84] Creating CNI manager for ""
I0513 11:28:50.103236    1464 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0513 11:28:50.103268    1464 start.go:340] cluster config:
{Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/kauls:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0513 11:28:50.106116    1464 out.go:177] 👍  Starting "minikube" primary control-plane node in "minikube" cluster
I0513 11:28:50.108728    1464 cache.go:121] Beginning downloading kic base image for docker with docker
I0513 11:28:50.111340    1464 out.go:177] 🚜  Pulling base image v0.0.43 ...
I0513 11:28:50.114213    1464 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0513 11:28:50.114243    1464 preload.go:147] Found local preload: /home/kauls/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4
I0513 11:28:50.114248    1464 cache.go:56] Caching tarball of preloaded images
I0513 11:28:50.114293    1464 image.go:79] Checking for gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon
I0513 11:28:50.114404    1464 preload.go:173] Found /home/kauls/.minikube/cache/preloaded-tarball/preloaded-images-k8s-v18-v1.30.0-docker-overlay2-amd64.tar.lz4 in cache, skipping download
I0513 11:28:50.114409    1464 cache.go:59] Finished verifying existence of preloaded tar for v1.30.0 on docker
I0513 11:28:50.114464    1464 profile.go:143] Saving config to /home/kauls/.minikube/profiles/minikube/config.json ...
I0513 11:28:50.125647    1464 image.go:83] Found gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 in local docker daemon, skipping pull
I0513 11:28:50.125657    1464 cache.go:144] gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 exists in daemon, skipping load
I0513 11:28:50.125682    1464 cache.go:194] Successfully downloaded all kic artifacts
I0513 11:28:50.125702    1464 start.go:360] acquireMachinesLock for minikube: {Name:mkfca939bf4ea3839e23c83db8d7983e7ce005e3 Clock:{} Delay:500ms Timeout:10m0s Cancel:<nil>}
I0513 11:28:50.125797    1464 start.go:364] duration metric: took 72.9µs to acquireMachinesLock for "minikube"
I0513 11:28:50.125808    1464 start.go:96] Skipping create...Using existing machine configuration
I0513 11:28:50.125810    1464 fix.go:54] fixHost starting: 
I0513 11:28:50.125944    1464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 11:28:50.138929    1464 fix.go:112] recreateIfNeeded on minikube: state=Stopped err=<nil>
W0513 11:28:50.138951    1464 fix.go:138] unexpected machine state, will restart: <nil>
I0513 11:28:50.141898    1464 out.go:177] 🔄  Restarting existing docker container for "minikube" ...
I0513 11:28:50.144551    1464 cli_runner.go:164] Run: docker start minikube
I0513 11:28:50.525592    1464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 11:28:50.536838    1464 kic.go:430] container "minikube" state is running.
I0513 11:28:50.537041    1464 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0513 11:28:50.549270    1464 profile.go:143] Saving config to /home/kauls/.minikube/profiles/minikube/config.json ...
I0513 11:28:50.549382    1464 machine.go:94] provisionDockerMachine start ...
I0513 11:28:50.549414    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:50.562242    1464 main.go:141] libmachine: Using SSH client type: native
I0513 11:28:50.562689    1464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0513 11:28:50.562695    1464 main.go:141] libmachine: About to run SSH command:
hostname
I0513 11:28:50.563247    1464 main.go:141] libmachine: Error dialing TCP: ssh: handshake failed: read tcp 127.0.0.1:35934->127.0.0.1:32772: read: connection reset by peer
I0513 11:28:53.705305    1464 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0513 11:28:53.705316    1464 ubuntu.go:169] provisioning hostname "minikube"
I0513 11:28:53.705392    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:53.719711    1464 main.go:141] libmachine: Using SSH client type: native
I0513 11:28:53.719815    1464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0513 11:28:53.719820    1464 main.go:141] libmachine: About to run SSH command:
sudo hostname minikube && echo "minikube" | sudo tee /etc/hostname
I0513 11:28:53.852836    1464 main.go:141] libmachine: SSH cmd err, output: <nil>: minikube

I0513 11:28:53.852877    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:53.864425    1464 main.go:141] libmachine: Using SSH client type: native
I0513 11:28:53.864535    1464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0513 11:28:53.864544    1464 main.go:141] libmachine: About to run SSH command:

		if ! grep -xq '.*\sminikube' /etc/hosts; then
			if grep -xq '127.0.1.1\s.*' /etc/hosts; then
				sudo sed -i 's/^127.0.1.1\s.*/127.0.1.1 minikube/g' /etc/hosts;
			else 
				echo '127.0.1.1 minikube' | sudo tee -a /etc/hosts; 
			fi
		fi
I0513 11:28:53.975193    1464 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0513 11:28:53.975235    1464 ubuntu.go:175] set auth options {CertDir:/home/kauls/.minikube CaCertPath:/home/kauls/.minikube/certs/ca.pem CaPrivateKeyPath:/home/kauls/.minikube/certs/ca-key.pem CaCertRemotePath:/etc/docker/ca.pem ServerCertPath:/home/kauls/.minikube/machines/server.pem ServerKeyPath:/home/kauls/.minikube/machines/server-key.pem ClientKeyPath:/home/kauls/.minikube/certs/key.pem ServerCertRemotePath:/etc/docker/server.pem ServerKeyRemotePath:/etc/docker/server-key.pem ClientCertPath:/home/kauls/.minikube/certs/cert.pem ServerCertSANs:[] StorePath:/home/kauls/.minikube}
I0513 11:28:53.975291    1464 ubuntu.go:177] setting up certificates
I0513 11:28:53.975307    1464 provision.go:84] configureAuth start
I0513 11:28:53.975383    1464 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0513 11:28:53.994940    1464 provision.go:143] copyHostCerts
I0513 11:28:53.996737    1464 exec_runner.go:144] found /home/kauls/.minikube/key.pem, removing ...
I0513 11:28:53.996745    1464 exec_runner.go:203] rm: /home/kauls/.minikube/key.pem
I0513 11:28:53.996781    1464 exec_runner.go:151] cp: /home/kauls/.minikube/certs/key.pem --> /home/kauls/.minikube/key.pem (1679 bytes)
I0513 11:28:53.997115    1464 exec_runner.go:144] found /home/kauls/.minikube/ca.pem, removing ...
I0513 11:28:53.997118    1464 exec_runner.go:203] rm: /home/kauls/.minikube/ca.pem
I0513 11:28:53.997138    1464 exec_runner.go:151] cp: /home/kauls/.minikube/certs/ca.pem --> /home/kauls/.minikube/ca.pem (1074 bytes)
I0513 11:28:53.997404    1464 exec_runner.go:144] found /home/kauls/.minikube/cert.pem, removing ...
I0513 11:28:53.997408    1464 exec_runner.go:203] rm: /home/kauls/.minikube/cert.pem
I0513 11:28:53.997429    1464 exec_runner.go:151] cp: /home/kauls/.minikube/certs/cert.pem --> /home/kauls/.minikube/cert.pem (1119 bytes)
I0513 11:28:53.997668    1464 provision.go:117] generating server cert: /home/kauls/.minikube/machines/server.pem ca-key=/home/kauls/.minikube/certs/ca.pem private-key=/home/kauls/.minikube/certs/ca-key.pem org=kauls.minikube san=[127.0.0.1 192.168.49.2 localhost minikube]
I0513 11:28:54.200735    1464 provision.go:177] copyRemoteCerts
I0513 11:28:54.200763    1464 ssh_runner.go:195] Run: sudo mkdir -p /etc/docker /etc/docker /etc/docker
I0513 11:28:54.200783    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:54.214482    1464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/kauls/.minikube/machines/minikube/id_rsa Username:docker}
I0513 11:28:54.306997    1464 ssh_runner.go:362] scp /home/kauls/.minikube/certs/ca.pem --> /etc/docker/ca.pem (1074 bytes)
I0513 11:28:54.324562    1464 ssh_runner.go:362] scp /home/kauls/.minikube/machines/server.pem --> /etc/docker/server.pem (1176 bytes)
I0513 11:28:54.341200    1464 ssh_runner.go:362] scp /home/kauls/.minikube/machines/server-key.pem --> /etc/docker/server-key.pem (1679 bytes)
I0513 11:28:54.357091    1464 provision.go:87] duration metric: took 381.775996ms to configureAuth
I0513 11:28:54.357117    1464 ubuntu.go:193] setting minikube options for container-runtime
I0513 11:28:54.357276    1464 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0513 11:28:54.357315    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:54.372746    1464 main.go:141] libmachine: Using SSH client type: native
I0513 11:28:54.372895    1464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0513 11:28:54.372900    1464 main.go:141] libmachine: About to run SSH command:
df --output=fstype / | tail -n 1
I0513 11:28:54.479116    1464 main.go:141] libmachine: SSH cmd err, output: <nil>: overlay

I0513 11:28:54.479124    1464 ubuntu.go:71] root file system type: overlay
I0513 11:28:54.479209    1464 provision.go:314] Updating docker unit: /lib/systemd/system/docker.service ...
I0513 11:28:54.479242    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:54.492540    1464 main.go:141] libmachine: Using SSH client type: native
I0513 11:28:54.492654    1464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0513 11:28:54.492698    1464 main.go:141] libmachine: About to run SSH command:
sudo mkdir -p /lib/systemd/system && printf %!s(MISSING) "[Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP \$MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target
" | sudo tee /lib/systemd/system/docker.service.new
I0513 11:28:54.605126    1464 main.go:141] libmachine: SSH cmd err, output: <nil>: [Unit]
Description=Docker Application Container Engine
Documentation=https://docs.docker.com
BindsTo=containerd.service
After=network-online.target firewalld.service containerd.service
Wants=network-online.target
Requires=docker.socket
StartLimitBurst=3
StartLimitIntervalSec=60

[Service]
Type=notify
Restart=on-failure



# This file is a systemd drop-in unit that inherits from the base dockerd configuration.
# The base configuration already specifies an 'ExecStart=...' command. The first directive
# here is to clear out that command inherited from the base configuration. Without this,
# the command from the base configuration and the command specified here are treated as
# a sequence of commands, which is not the desired behavior, nor is it valid -- systemd
# will catch this invalid input and refuse to start the service with an error like:
#  Service has more than one ExecStart= setting, which is only allowed for Type=oneshot services.

# NOTE: default-ulimit=nofile is set to an arbitrary number for consistency with other
# container runtimes. If left unlimited, it may result in OOM issues with MySQL.
ExecStart=
ExecStart=/usr/bin/dockerd -H tcp://0.0.0.0:2376 -H unix:///var/run/docker.sock --default-ulimit=nofile=1048576:1048576 --tlsverify --tlscacert /etc/docker/ca.pem --tlscert /etc/docker/server.pem --tlskey /etc/docker/server-key.pem --label provider=docker --insecure-registry 10.96.0.0/12 
ExecReload=/bin/kill -s HUP $MAINPID

# Having non-zero Limit*s causes performance problems due to accounting overhead
# in the kernel. We recommend using cgroups to do container-local accounting.
LimitNOFILE=infinity
LimitNPROC=infinity
LimitCORE=infinity

# Uncomment TasksMax if your systemd version supports it.
# Only systemd 226 and above support this version.
TasksMax=infinity
TimeoutStartSec=0

# set delegate yes so that systemd does not reset the cgroups of docker containers
Delegate=yes

# kill only the docker process, not all processes in the cgroup
KillMode=process

[Install]
WantedBy=multi-user.target

I0513 11:28:54.605284    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:54.619075    1464 main.go:141] libmachine: Using SSH client type: native
I0513 11:28:54.619184    1464 main.go:141] libmachine: &{{{<nil> 0 [] [] []} docker [0x82d4e0] 0x830240 <nil>  [] 0s} 127.0.0.1 32772 <nil> <nil>}
I0513 11:28:54.619194    1464 main.go:141] libmachine: About to run SSH command:
sudo diff -u /lib/systemd/system/docker.service /lib/systemd/system/docker.service.new || { sudo mv /lib/systemd/system/docker.service.new /lib/systemd/system/docker.service; sudo systemctl -f daemon-reload && sudo systemctl -f enable docker && sudo systemctl -f restart docker; }
I0513 11:28:54.740212    1464 main.go:141] libmachine: SSH cmd err, output: <nil>: 
I0513 11:28:54.740222    1464 machine.go:97] duration metric: took 4.190834525s to provisionDockerMachine
I0513 11:28:54.740228    1464 start.go:293] postStartSetup for "minikube" (driver="docker")
I0513 11:28:54.740235    1464 start.go:322] creating required directories: [/etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs]
I0513 11:28:54.740267    1464 ssh_runner.go:195] Run: sudo mkdir -p /etc/kubernetes/addons /etc/kubernetes/manifests /var/tmp/minikube /var/lib/minikube /var/lib/minikube/certs /var/lib/minikube/images /var/lib/minikube/binaries /tmp/gvisor /usr/share/ca-certificates /etc/ssl/certs
I0513 11:28:54.740305    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:54.752125    1464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/kauls/.minikube/machines/minikube/id_rsa Username:docker}
I0513 11:28:54.850469    1464 ssh_runner.go:195] Run: cat /etc/os-release
I0513 11:28:54.852856    1464 main.go:141] libmachine: Couldn't set key VERSION_CODENAME, no corresponding struct field found
I0513 11:28:54.852869    1464 main.go:141] libmachine: Couldn't set key PRIVACY_POLICY_URL, no corresponding struct field found
I0513 11:28:54.852873    1464 main.go:141] libmachine: Couldn't set key UBUNTU_CODENAME, no corresponding struct field found
I0513 11:28:54.852876    1464 info.go:137] Remote host: Ubuntu 22.04.4 LTS
I0513 11:28:54.852881    1464 filesync.go:126] Scanning /home/kauls/.minikube/addons for local assets ...
I0513 11:28:54.853184    1464 filesync.go:126] Scanning /home/kauls/.minikube/files for local assets ...
I0513 11:28:54.853485    1464 start.go:296] duration metric: took 113.251106ms for postStartSetup
I0513 11:28:54.853510    1464 ssh_runner.go:195] Run: sh -c "df -h /var | awk 'NR==2{print $5}'"
I0513 11:28:54.853528    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:54.865939    1464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/kauls/.minikube/machines/minikube/id_rsa Username:docker}
I0513 11:28:54.948580    1464 ssh_runner.go:195] Run: sh -c "df -BG /var | awk 'NR==2{print $4}'"
I0513 11:28:54.952527    1464 fix.go:56] duration metric: took 4.826710821s for fixHost
I0513 11:28:54.952539    1464 start.go:83] releasing machines lock for "minikube", held for 4.826736121s
I0513 11:28:54.952615    1464 cli_runner.go:164] Run: docker container inspect -f "{{range .NetworkSettings.Networks}}{{.IPAddress}},{{.GlobalIPv6Address}}{{end}}" minikube
I0513 11:28:54.968684    1464 ssh_runner.go:195] Run: cat /version.json
I0513 11:28:54.968713    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:54.968789    1464 ssh_runner.go:195] Run: curl -sS -m 2 https://registry.k8s.io/
I0513 11:28:54.968821    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:28:54.982035    1464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/kauls/.minikube/machines/minikube/id_rsa Username:docker}
I0513 11:28:54.984714    1464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/kauls/.minikube/machines/minikube/id_rsa Username:docker}
I0513 11:29:03.101852    1464 ssh_runner.go:235] Completed: curl -sS -m 2 https://registry.k8s.io/: (8.133026099s)
W0513 11:29:03.101883    1464 start.go:860] [curl -sS -m 2 https://registry.k8s.io/] failed: curl -sS -m 2 https://registry.k8s.io/: Process exited with status 28
stdout:

stderr:
curl: (28) Resolving timed out after 2000 milliseconds
W0513 11:29:03.101924    1464 out.go:239] ❗  This container is having trouble accessing https://registry.k8s.io
I0513 11:29:03.101939    1464 ssh_runner.go:235] Completed: cat /version.json: (8.1332408s)
W0513 11:29:03.101973    1464 out.go:239] 💡  To pull new external images, you may need to configure a proxy: https://minikube.sigs.k8s.io/docs/reference/networking/proxy/
I0513 11:29:03.102025    1464 ssh_runner.go:195] Run: systemctl --version
I0513 11:29:03.108864    1464 ssh_runner.go:195] Run: sh -c "stat /etc/cni/net.d/*loopback.conf*"
I0513 11:29:03.112259    1464 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f -name *loopback.conf* -not -name *.mk_disabled -exec sh -c "grep -q loopback {} && ( grep -q name {} || sudo sed -i '/"type": "loopback"/i \ \ \ \ "name": "loopback",' {} ) && sudo sed -i 's|"cniVersion": ".*"|"cniVersion": "1.0.0"|g' {}" ;
I0513 11:29:03.125911    1464 cni.go:230] loopback cni configuration patched: "/etc/cni/net.d/*loopback.conf*" found
I0513 11:29:03.125948    1464 ssh_runner.go:195] Run: sudo find /etc/cni/net.d -maxdepth 1 -type f ( ( -name *bridge* -or -name *podman* ) -and -not -name *.mk_disabled ) -printf "%!p(MISSING), " -exec sh -c "sudo mv {} {}.mk_disabled" ;
I0513 11:29:03.132043    1464 cni.go:259] no active bridge cni configs found in "/etc/cni/net.d" - nothing to disable
I0513 11:29:03.132059    1464 start.go:494] detecting cgroup driver to use...
I0513 11:29:03.132082    1464 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0513 11:29:03.132195    1464 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///run/containerd/containerd.sock
" | sudo tee /etc/crictl.yaml"
I0513 11:29:03.143052    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)sandbox_image = .*$|\1sandbox_image = "registry.k8s.io/pause:3.9"|' /etc/containerd/config.toml"
I0513 11:29:03.150960    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)restrict_oom_score_adj = .*$|\1restrict_oom_score_adj = false|' /etc/containerd/config.toml"
I0513 11:29:03.158683    1464 containerd.go:146] configuring containerd to use "cgroupfs" as cgroup driver...
I0513 11:29:03.158715    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)SystemdCgroup = .*$|\1SystemdCgroup = false|g' /etc/containerd/config.toml"
I0513 11:29:03.165582    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runtime.v1.linux"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0513 11:29:03.172882    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i '/systemd_cgroup/d' /etc/containerd/config.toml"
I0513 11:29:03.180321    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i 's|"io.containerd.runc.v1"|"io.containerd.runc.v2"|g' /etc/containerd/config.toml"
I0513 11:29:03.188017    1464 ssh_runner.go:195] Run: sh -c "sudo rm -rf /etc/cni/net.mk"
I0513 11:29:03.194610    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)conf_dir = .*$|\1conf_dir = "/etc/cni/net.d"|g' /etc/containerd/config.toml"
I0513 11:29:03.200995    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i '/^ *enable_unprivileged_ports = .*/d' /etc/containerd/config.toml"
I0513 11:29:03.207271    1464 ssh_runner.go:195] Run: sh -c "sudo sed -i -r 's|^( *)\[plugins."io.containerd.grpc.v1.cri"\]|&\n\1  enable_unprivileged_ports = true|' /etc/containerd/config.toml"
I0513 11:29:03.213293    1464 ssh_runner.go:195] Run: sudo sysctl net.bridge.bridge-nf-call-iptables
I0513 11:29:03.219621    1464 ssh_runner.go:195] Run: sudo sh -c "echo 1 > /proc/sys/net/ipv4/ip_forward"
I0513 11:29:03.224820    1464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 11:29:03.318689    1464 ssh_runner.go:195] Run: sudo systemctl restart containerd
I0513 11:29:03.419889    1464 start.go:494] detecting cgroup driver to use...
I0513 11:29:03.419929    1464 detect.go:196] detected "cgroupfs" cgroup driver on host os
I0513 11:29:03.419974    1464 ssh_runner.go:195] Run: sudo systemctl cat docker.service
I0513 11:29:03.428794    1464 cruntime.go:279] skipping containerd shutdown because we are bound to it
I0513 11:29:03.428825    1464 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service crio
I0513 11:29:03.438177    1464 ssh_runner.go:195] Run: /bin/bash -c "sudo mkdir -p /etc && printf %!s(MISSING) "runtime-endpoint: unix:///var/run/cri-dockerd.sock
" | sudo tee /etc/crictl.yaml"
I0513 11:29:03.449979    1464 ssh_runner.go:195] Run: which cri-dockerd
I0513 11:29:03.452258    1464 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/cri-docker.service.d
I0513 11:29:03.458178    1464 ssh_runner.go:362] scp memory --> /etc/systemd/system/cri-docker.service.d/10-cni.conf (189 bytes)
I0513 11:29:03.469968    1464 ssh_runner.go:195] Run: sudo systemctl unmask docker.service
I0513 11:29:03.555731    1464 ssh_runner.go:195] Run: sudo systemctl enable docker.socket
I0513 11:29:03.663706    1464 docker.go:574] configuring docker to use "cgroupfs" as cgroup driver...
I0513 11:29:03.663794    1464 ssh_runner.go:362] scp memory --> /etc/docker/daemon.json (130 bytes)
I0513 11:29:03.678557    1464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 11:29:03.779550    1464 ssh_runner.go:195] Run: sudo systemctl restart docker
I0513 11:29:06.460912    1464 ssh_runner.go:235] Completed: sudo systemctl restart docker: (2.681344628s)
I0513 11:29:06.460948    1464 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.socket
I0513 11:29:06.469245    1464 ssh_runner.go:195] Run: sudo systemctl stop cri-docker.socket
I0513 11:29:06.478049    1464 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0513 11:29:06.486451    1464 ssh_runner.go:195] Run: sudo systemctl unmask cri-docker.socket
I0513 11:29:06.578225    1464 ssh_runner.go:195] Run: sudo systemctl enable cri-docker.socket
I0513 11:29:06.657705    1464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 11:29:06.698239    1464 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.socket
I0513 11:29:06.707374    1464 ssh_runner.go:195] Run: sudo systemctl is-active --quiet service cri-docker.service
I0513 11:29:06.714578    1464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 11:29:06.791982    1464 ssh_runner.go:195] Run: sudo systemctl restart cri-docker.service
I0513 11:29:06.986932    1464 start.go:541] Will wait 60s for socket path /var/run/cri-dockerd.sock
I0513 11:29:06.986967    1464 ssh_runner.go:195] Run: stat /var/run/cri-dockerd.sock
I0513 11:29:06.989464    1464 start.go:562] Will wait 60s for crictl version
I0513 11:29:06.989496    1464 ssh_runner.go:195] Run: which crictl
I0513 11:29:06.991767    1464 ssh_runner.go:195] Run: sudo /usr/bin/crictl version
I0513 11:29:07.091026    1464 start.go:578] Version:  0.1.0
RuntimeName:  docker
RuntimeVersion:  26.0.1
RuntimeApiVersion:  v1
I0513 11:29:07.091060    1464 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0513 11:29:07.163819    1464 ssh_runner.go:195] Run: docker version --format {{.Server.Version}}
I0513 11:29:07.182943    1464 out.go:204] 🐳  Preparing Kubernetes v1.30.0 on Docker 26.0.1 ...
I0513 11:29:07.183027    1464 cli_runner.go:164] Run: docker network inspect minikube --format "{"Name": "{{.Name}}","Driver": "{{.Driver}}","Subnet": "{{range .IPAM.Config}}{{.Subnet}}{{end}}","Gateway": "{{range .IPAM.Config}}{{.Gateway}}{{end}}","MTU": {{if (index .Options "com.docker.network.driver.mtu")}}{{(index .Options "com.docker.network.driver.mtu")}}{{else}}0{{end}}, "ContainerIPs": [{{range $k,$v := .Containers }}"{{$v.IPv4Address}}",{{end}}]}"
I0513 11:29:07.194875    1464 ssh_runner.go:195] Run: grep 192.168.49.1	host.minikube.internal$ /etc/hosts
I0513 11:29:07.197257    1464 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\thost.minikube.internal$' "/etc/hosts"; echo "192.168.49.1	host.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0513 11:29:07.204460    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0513 11:29:07.216021    1464 kubeadm.go:877] updating cluster {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/kauls:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s} ...
I0513 11:29:07.216099    1464 preload.go:132] Checking if preload exists for k8s version v1.30.0 and runtime docker
I0513 11:29:07.216130    1464 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0513 11:29:07.229645    1464 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0513 11:29:07.229653    1464 docker.go:615] Images already preloaded, skipping extraction
I0513 11:29:07.229687    1464 ssh_runner.go:195] Run: docker images --format {{.Repository}}:{{.Tag}}
I0513 11:29:07.243202    1464 docker.go:685] Got preloaded images: -- stdout --
registry.k8s.io/kube-apiserver:v1.30.0
registry.k8s.io/kube-scheduler:v1.30.0
registry.k8s.io/kube-controller-manager:v1.30.0
registry.k8s.io/kube-proxy:v1.30.0
registry.k8s.io/etcd:3.5.12-0
registry.k8s.io/coredns/coredns:v1.11.1
registry.k8s.io/pause:3.9
gcr.io/k8s-minikube/storage-provisioner:v5

-- /stdout --
I0513 11:29:07.243210    1464 cache_images.go:84] Images are preloaded, skipping loading
I0513 11:29:07.243216    1464 kubeadm.go:928] updating node { 192.168.49.2 8443 v1.30.0 docker true true} ...
I0513 11:29:07.243289    1464 kubeadm.go:940] kubelet [Unit]
Wants=docker.socket

[Service]
ExecStart=
ExecStart=/var/lib/minikube/binaries/v1.30.0/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --config=/var/lib/kubelet/config.yaml --hostname-override=minikube --kubeconfig=/etc/kubernetes/kubelet.conf --node-ip=192.168.49.2

[Install]
 config:
{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:}
I0513 11:29:07.243319    1464 ssh_runner.go:195] Run: docker info --format {{.CgroupDriver}}
I0513 11:29:07.389279    1464 cni.go:84] Creating CNI manager for ""
I0513 11:29:07.389290    1464 cni.go:158] "docker" driver + "docker" container runtime found on kubernetes v1.24+, recommending bridge
I0513 11:29:07.389297    1464 kubeadm.go:84] Using pod CIDR: 10.244.0.0/16
I0513 11:29:07.389309    1464 kubeadm.go:181] kubeadm options: {CertDir:/var/lib/minikube/certs ServiceCIDR:10.96.0.0/12 PodSubnet:10.244.0.0/16 AdvertiseAddress:192.168.49.2 APIServerPort:8443 KubernetesVersion:v1.30.0 EtcdDataDir:/var/lib/minikube/etcd EtcdExtraArgs:map[] ClusterName:minikube NodeName:minikube DNSDomain:cluster.local CRISocket:/var/run/cri-dockerd.sock ImageRepository: ComponentOptions:[{Component:apiServer ExtraArgs:map[enable-admission-plugins:NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota] Pairs:map[certSANs:["127.0.0.1", "localhost", "192.168.49.2"]]} {Component:controllerManager ExtraArgs:map[allocate-node-cidrs:true leader-elect:false] Pairs:map[]} {Component:scheduler ExtraArgs:map[leader-elect:false] Pairs:map[]}] FeatureArgs:map[] NodeIP:192.168.49.2 CgroupDriver:cgroupfs ClientCAFile:/var/lib/minikube/certs/ca.crt StaticPodPath:/etc/kubernetes/manifests ControlPlaneAddress:control-plane.minikube.internal KubeProxyOptions:map[] ResolvConfSearchRegression:false KubeletConfigOpts:map[containerRuntimeEndpoint:unix:///var/run/cri-dockerd.sock hairpinMode:hairpin-veth runtimeRequestTimeout:15m] PrependCriSocketUnix:true}
I0513 11:29:07.389410    1464 kubeadm.go:187] kubeadm config:
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
localAPIEndpoint:
  advertiseAddress: 192.168.49.2
  bindPort: 8443
bootstrapTokens:
  - groups:
      - system:bootstrappers:kubeadm:default-node-token
    ttl: 24h0m0s
    usages:
      - signing
      - authentication
nodeRegistration:
  criSocket: unix:///var/run/cri-dockerd.sock
  name: "minikube"
  kubeletExtraArgs:
    node-ip: 192.168.49.2
  taints: []
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
apiServer:
  certSANs: ["127.0.0.1", "localhost", "192.168.49.2"]
  extraArgs:
    enable-admission-plugins: "NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,ResourceQuota"
controllerManager:
  extraArgs:
    allocate-node-cidrs: "true"
    leader-elect: "false"
scheduler:
  extraArgs:
    leader-elect: "false"
certificatesDir: /var/lib/minikube/certs
clusterName: mk
controlPlaneEndpoint: control-plane.minikube.internal:8443
etcd:
  local:
    dataDir: /var/lib/minikube/etcd
    extraArgs:
      proxy-refresh-interval: "70000"
kubernetesVersion: v1.30.0
networking:
  dnsDomain: cluster.local
  podSubnet: "10.244.0.0/16"
  serviceSubnet: 10.96.0.0/12
---
apiVersion: kubelet.config.k8s.io/v1beta1
kind: KubeletConfiguration
authentication:
  x509:
    clientCAFile: /var/lib/minikube/certs/ca.crt
cgroupDriver: cgroupfs
containerRuntimeEndpoint: unix:///var/run/cri-dockerd.sock
hairpinMode: hairpin-veth
runtimeRequestTimeout: 15m
clusterDomain: "cluster.local"
# disable disk resource management by default
imageGCHighThresholdPercent: 100
evictionHard:
  nodefs.available: "0%!"(MISSING)
  nodefs.inodesFree: "0%!"(MISSING)
  imagefs.available: "0%!"(MISSING)
failSwapOn: false
staticPodPath: /etc/kubernetes/manifests
---
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
clusterCIDR: "10.244.0.0/16"
metricsBindAddress: 0.0.0.0:10249
conntrack:
  maxPerCore: 0
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_established"
  tcpEstablishedTimeout: 0s
# Skip setting "net.netfilter.nf_conntrack_tcp_timeout_close"
  tcpCloseWaitTimeout: 0s

I0513 11:29:07.389450    1464 ssh_runner.go:195] Run: sudo ls /var/lib/minikube/binaries/v1.30.0
I0513 11:29:07.396668    1464 binaries.go:44] Found k8s binaries, skipping transfer
I0513 11:29:07.396697    1464 ssh_runner.go:195] Run: sudo mkdir -p /etc/systemd/system/kubelet.service.d /lib/systemd/system /var/tmp/minikube
I0513 11:29:07.401885    1464 ssh_runner.go:362] scp memory --> /etc/systemd/system/kubelet.service.d/10-kubeadm.conf (307 bytes)
I0513 11:29:07.413069    1464 ssh_runner.go:362] scp memory --> /lib/systemd/system/kubelet.service (352 bytes)
I0513 11:29:07.423752    1464 ssh_runner.go:362] scp memory --> /var/tmp/minikube/kubeadm.yaml.new (2150 bytes)
I0513 11:29:07.437943    1464 ssh_runner.go:195] Run: grep 192.168.49.2	control-plane.minikube.internal$ /etc/hosts
I0513 11:29:07.440296    1464 ssh_runner.go:195] Run: /bin/bash -c "{ grep -v $'\tcontrol-plane.minikube.internal$' "/etc/hosts"; echo "192.168.49.2	control-plane.minikube.internal"; } > /tmp/h.$$; sudo cp /tmp/h.$$ "/etc/hosts""
I0513 11:29:07.447227    1464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 11:29:07.540305    1464 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0513 11:29:07.550922    1464 certs.go:68] Setting up /home/kauls/.minikube/profiles/minikube for IP: 192.168.49.2
I0513 11:29:07.550929    1464 certs.go:194] generating shared ca certs ...
I0513 11:29:07.550940    1464 certs.go:226] acquiring lock for ca certs: {Name:mk5fece5258ed6685d0397080df18cfe0b666c98 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 11:29:07.551642    1464 certs.go:235] skipping valid "minikubeCA" ca cert: /home/kauls/.minikube/ca.key
I0513 11:29:07.552005    1464 certs.go:235] skipping valid "proxyClientCA" ca cert: /home/kauls/.minikube/proxy-client-ca.key
I0513 11:29:07.552032    1464 certs.go:256] generating profile certs ...
I0513 11:29:07.552384    1464 certs.go:359] skipping valid signed profile cert regeneration for "minikube-user": /home/kauls/.minikube/profiles/minikube/client.key
I0513 11:29:07.552713    1464 certs.go:359] skipping valid signed profile cert regeneration for "minikube": /home/kauls/.minikube/profiles/minikube/apiserver.key.7fb57e3c
I0513 11:29:07.553004    1464 certs.go:359] skipping valid signed profile cert regeneration for "aggregator": /home/kauls/.minikube/profiles/minikube/proxy-client.key
I0513 11:29:07.553111    1464 certs.go:484] found cert: /home/kauls/.minikube/certs/ca-key.pem (1679 bytes)
I0513 11:29:07.553126    1464 certs.go:484] found cert: /home/kauls/.minikube/certs/ca.pem (1074 bytes)
I0513 11:29:07.553161    1464 certs.go:484] found cert: /home/kauls/.minikube/certs/cert.pem (1119 bytes)
I0513 11:29:07.553171    1464 certs.go:484] found cert: /home/kauls/.minikube/certs/key.pem (1679 bytes)
I0513 11:29:07.553593    1464 ssh_runner.go:362] scp /home/kauls/.minikube/ca.crt --> /var/lib/minikube/certs/ca.crt (1111 bytes)
I0513 11:29:07.570961    1464 ssh_runner.go:362] scp /home/kauls/.minikube/ca.key --> /var/lib/minikube/certs/ca.key (1679 bytes)
I0513 11:29:07.589806    1464 ssh_runner.go:362] scp /home/kauls/.minikube/proxy-client-ca.crt --> /var/lib/minikube/certs/proxy-client-ca.crt (1119 bytes)
I0513 11:29:07.608761    1464 ssh_runner.go:362] scp /home/kauls/.minikube/proxy-client-ca.key --> /var/lib/minikube/certs/proxy-client-ca.key (1675 bytes)
I0513 11:29:07.625988    1464 ssh_runner.go:362] scp /home/kauls/.minikube/profiles/minikube/apiserver.crt --> /var/lib/minikube/certs/apiserver.crt (1411 bytes)
I0513 11:29:07.642265    1464 ssh_runner.go:362] scp /home/kauls/.minikube/profiles/minikube/apiserver.key --> /var/lib/minikube/certs/apiserver.key (1675 bytes)
I0513 11:29:07.658593    1464 ssh_runner.go:362] scp /home/kauls/.minikube/profiles/minikube/proxy-client.crt --> /var/lib/minikube/certs/proxy-client.crt (1147 bytes)
I0513 11:29:07.675237    1464 ssh_runner.go:362] scp /home/kauls/.minikube/profiles/minikube/proxy-client.key --> /var/lib/minikube/certs/proxy-client.key (1679 bytes)
I0513 11:29:07.692833    1464 ssh_runner.go:362] scp /home/kauls/.minikube/ca.crt --> /usr/share/ca-certificates/minikubeCA.pem (1111 bytes)
I0513 11:29:07.709519    1464 ssh_runner.go:362] scp memory --> /var/lib/minikube/kubeconfig (738 bytes)
I0513 11:29:07.721955    1464 ssh_runner.go:195] Run: openssl version
I0513 11:29:07.729461    1464 ssh_runner.go:195] Run: sudo /bin/bash -c "test -s /usr/share/ca-certificates/minikubeCA.pem && ln -fs /usr/share/ca-certificates/minikubeCA.pem /etc/ssl/certs/minikubeCA.pem"
I0513 11:29:07.739847    1464 ssh_runner.go:195] Run: ls -la /usr/share/ca-certificates/minikubeCA.pem
I0513 11:29:07.742154    1464 certs.go:528] hashing: -rw-r--r-- 1 root root 1111 May  3 21:33 /usr/share/ca-certificates/minikubeCA.pem
I0513 11:29:07.742177    1464 ssh_runner.go:195] Run: openssl x509 -hash -noout -in /usr/share/ca-certificates/minikubeCA.pem
I0513 11:29:07.746726    1464 ssh_runner.go:195] Run: sudo /bin/bash -c "test -L /etc/ssl/certs/b5213941.0 || ln -fs /etc/ssl/certs/minikubeCA.pem /etc/ssl/certs/b5213941.0"
I0513 11:29:07.753194    1464 ssh_runner.go:195] Run: stat /var/lib/minikube/certs/apiserver-kubelet-client.crt
I0513 11:29:07.755542    1464 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-etcd-client.crt -checkend 86400
I0513 11:29:07.760314    1464 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/apiserver-kubelet-client.crt -checkend 86400
I0513 11:29:07.765427    1464 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/server.crt -checkend 86400
I0513 11:29:07.770620    1464 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/healthcheck-client.crt -checkend 86400
I0513 11:29:07.775780    1464 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/etcd/peer.crt -checkend 86400
I0513 11:29:07.780730    1464 ssh_runner.go:195] Run: openssl x509 -noout -in /var/lib/minikube/certs/front-proxy-client.crt -checkend 86400
I0513 11:29:07.787584    1464 kubeadm.go:391] StartCluster: {Name:minikube KeepContext:false EmbedCerts:false MinikubeISO: KicBaseImage:gcr.io/k8s-minikube/kicbase:v0.0.43@sha256:7ff490df401cc0fbf19a4521544ae8f4a00cc163e92a95017a8d8bfdb1422737 Memory:3900 CPUs:2 DiskSize:20000 Driver:docker HyperkitVpnKitSock: HyperkitVSockPorts:[] DockerEnv:[] ContainerVolumeMounts:[] InsecureRegistry:[] RegistryMirror:[] HostOnlyCIDR:192.168.59.1/24 HypervVirtualSwitch: HypervUseExternalSwitch:false HypervExternalAdapter: KVMNetwork:default KVMQemuURI:qemu:///system KVMGPU:false KVMHidden:false KVMNUMACount:1 APIServerPort:8443 DockerOpt:[] DisableDriverMounts:false NFSShare:[] NFSSharesRoot:/nfsshares UUID: NoVTXCheck:false DNSProxy:false HostDNSResolver:true HostOnlyNicType:virtio NatNicType:virtio SSHIPAddress: SSHUser:root SSHKey: SSHPort:22 KubernetesConfig:{KubernetesVersion:v1.30.0 ClusterName:minikube Namespace:default APIServerHAVIP: APIServerName:minikubeCA APIServerNames:[] APIServerIPs:[] DNSDomain:cluster.local ContainerRuntime:docker CRISocket: NetworkPlugin:cni FeatureGates: ServiceCIDR:10.96.0.0/12 ImageRepository: LoadBalancerStartIP: LoadBalancerEndIP: CustomIngressCert: RegistryAliases: ExtraOptions:[] ShouldLoadCachedImages:true EnableDefaultCNI:false CNI:} Nodes:[{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}] Addons:map[default-storageclass:true storage-provisioner:true] CustomAddonImages:map[] CustomAddonRegistries:map[] VerifyComponents:map[apiserver:true system_pods:true] StartHostTimeout:6m0s ScheduledStop:<nil> ExposedPorts:[] ListenAddress: Network: Subnet: MultiNodeRequested:false ExtraDisks:0 CertExpiration:26280h0m0s Mount:false MountString:/home/kauls:/minikube-host Mount9PVersion:9p2000.L MountGID:docker MountIP: MountMSize:262144 MountOptions:[] MountPort:0 MountType:9p MountUID:docker BinaryMirror: DisableOptimizations:false DisableMetrics:false CustomQemuFirmwarePath: SocketVMnetClientPath: SocketVMnetPath: StaticIP: SSHAuthSock: SSHAgentPID:0 GPUs: AutoPauseInterval:1m0s}
I0513 11:29:07.787659    1464 ssh_runner.go:195] Run: docker ps --filter status=paused --filter=name=k8s_.*_(kube-system)_ --format={{.ID}}
I0513 11:29:07.805060    1464 ssh_runner.go:195] Run: sudo ls /var/lib/kubelet/kubeadm-flags.env /var/lib/kubelet/config.yaml /var/lib/minikube/etcd
W0513 11:29:07.812754    1464 kubeadm.go:404] apiserver tunnel failed: apiserver port not set
I0513 11:29:07.812763    1464 kubeadm.go:407] found existing configuration files, will attempt cluster restart
I0513 11:29:07.812765    1464 kubeadm.go:587] restartPrimaryControlPlane start ...
I0513 11:29:07.812792    1464 ssh_runner.go:195] Run: sudo test -d /data/minikube
I0513 11:29:07.822871    1464 kubeadm.go:129] /data/minikube skipping compat symlinks: sudo test -d /data/minikube: Process exited with status 1
stdout:

stderr:
I0513 11:29:07.822921    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0513 11:29:07.840133    1464 kubeconfig.go:47] verify endpoint returned: get endpoint: "minikube" does not appear in /home/kauls/.kube/config
I0513 11:29:07.840245    1464 kubeconfig.go:62] /home/kauls/.kube/config needs updating (will repair): [kubeconfig missing "minikube" cluster setting kubeconfig missing "minikube" context setting]
I0513 11:29:07.840482    1464 lock.go:35] WriteFile acquiring /home/kauls/.kube/config: {Name:mkf2b2643fe7d83bdafe78d28fbab95ca5d48df1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 11:29:07.853719    1464 ssh_runner.go:195] Run: sudo diff -u /var/tmp/minikube/kubeadm.yaml /var/tmp/minikube/kubeadm.yaml.new
I0513 11:29:07.860964    1464 kubeadm.go:624] The running cluster does not require reconfiguration: 127.0.0.1
I0513 11:29:07.860981    1464 kubeadm.go:591] duration metric: took 48.213307ms to restartPrimaryControlPlane
I0513 11:29:07.860986    1464 kubeadm.go:393] duration metric: took 73.409811ms to StartCluster
I0513 11:29:07.860996    1464 settings.go:142] acquiring lock: {Name:mk50482191e98961497fbad6374160225b099f47 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 11:29:07.861075    1464 settings.go:150] Updating kubeconfig:  /home/kauls/.kube/config
I0513 11:29:07.861856    1464 lock.go:35] WriteFile acquiring /home/kauls/.kube/config: {Name:mkf2b2643fe7d83bdafe78d28fbab95ca5d48df1 Clock:{} Delay:500ms Timeout:1m0s Cancel:<nil>}
I0513 11:29:07.861996    1464 start.go:234] Will wait 6m0s for node &{Name: IP:192.168.49.2 Port:8443 KubernetesVersion:v1.30.0 ContainerRuntime:docker ControlPlane:true Worker:true}
I0513 11:29:07.865278    1464 out.go:177] 🔎  Verifying Kubernetes components...
I0513 11:29:07.862104    1464 config.go:182] Loaded profile config "minikube": Driver=docker, ContainerRuntime=docker, KubernetesVersion=v1.30.0
I0513 11:29:07.862089    1464 addons.go:502] enable addons start: toEnable=map[ambassador:false auto-pause:false cloud-spanner:false csi-hostpath-driver:false dashboard:false default-storageclass:true efk:false freshpod:false gcp-auth:false gvisor:false headlamp:false helm-tiller:false inaccel:false ingress:false ingress-dns:false inspektor-gadget:false istio:false istio-provisioner:false kong:false kubeflow:false kubevirt:false logviewer:false metallb:false metrics-server:false nvidia-device-plugin:false nvidia-driver-installer:false nvidia-gpu-device-plugin:false olm:false pod-security-policy:false portainer:false registry:false registry-aliases:false registry-creds:false storage-provisioner:true storage-provisioner-gluster:false storage-provisioner-rancher:false volumesnapshots:false yakd:false]
I0513 11:29:07.868422    1464 addons.go:69] Setting storage-provisioner=true in profile "minikube"
I0513 11:29:07.868442    1464 addons.go:234] Setting addon storage-provisioner=true in "minikube"
W0513 11:29:07.868448    1464 addons.go:243] addon storage-provisioner should already be in state true
I0513 11:29:07.868464    1464 ssh_runner.go:195] Run: sudo systemctl daemon-reload
I0513 11:29:07.868467    1464 host.go:66] Checking if "minikube" exists ...
I0513 11:29:07.868467    1464 addons.go:69] Setting default-storageclass=true in profile "minikube"
I0513 11:29:07.868484    1464 addons_storage_classes.go:33] enableOrDisableStorageClasses default-storageclass=true on "minikube"
I0513 11:29:07.868641    1464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 11:29:07.868686    1464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 11:29:07.887043    1464 out.go:177]     ▪ Using image gcr.io/k8s-minikube/storage-provisioner:v5
I0513 11:29:07.886059    1464 addons.go:234] Setting addon default-storageclass=true in "minikube"
W0513 11:29:07.889988    1464 addons.go:243] addon default-storageclass should already be in state true
I0513 11:29:07.890019    1464 host.go:66] Checking if "minikube" exists ...
I0513 11:29:07.890069    1464 addons.go:426] installing /etc/kubernetes/addons/storage-provisioner.yaml
I0513 11:29:07.890076    1464 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storage-provisioner.yaml (2676 bytes)
I0513 11:29:07.890107    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:29:07.890238    1464 cli_runner.go:164] Run: docker container inspect minikube --format={{.State.Status}}
I0513 11:29:07.903525    1464 addons.go:426] installing /etc/kubernetes/addons/storageclass.yaml
I0513 11:29:07.903534    1464 ssh_runner.go:362] scp memory --> /etc/kubernetes/addons/storageclass.yaml (271 bytes)
I0513 11:29:07.903569    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "22/tcp") 0).HostPort}}'" minikube
I0513 11:29:07.904765    1464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/kauls/.minikube/machines/minikube/id_rsa Username:docker}
I0513 11:29:07.917980    1464 sshutil.go:53] new ssh client: &{IP:127.0.0.1 Port:32772 SSHKeyPath:/home/kauls/.minikube/machines/minikube/id_rsa Username:docker}
I0513 11:29:08.138904    1464 ssh_runner.go:195] Run: sudo systemctl start kubelet
I0513 11:29:08.161181    1464 cli_runner.go:164] Run: docker container inspect -f "'{{(index (index .NetworkSettings.Ports "8443/tcp") 0).HostPort}}'" minikube
I0513 11:29:08.167954    1464 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml
I0513 11:29:08.169152    1464 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml
I0513 11:29:08.174152    1464 api_server.go:52] waiting for apiserver process to appear ...
I0513 11:29:08.174220    1464 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
W0513 11:29:08.660252    1464 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0513 11:29:08.660270    1464 retry.go:31] will retry after 216.320583ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
W0513 11:29:08.660348    1464 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0513 11:29:08.660355    1464 retry.go:31] will retry after 326.844747ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0513 11:29:08.674479    1464 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0513 11:29:08.877062    1464 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
W0513 11:29:08.923330    1464 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0513 11:29:08.923351    1464 retry.go:31] will retry after 365.958575ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storage-provisioner.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0513 11:29:08.987607    1464 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
W0513 11:29:09.051275    1464 addons.go:452] apply failed, will retry: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0513 11:29:09.051294    1464 retry.go:31] will retry after 376.649504ms: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: Process exited with status 1
stdout:

stderr:
error: error validating "/etc/kubernetes/addons/storageclass.yaml": error validating data: failed to download openapi: Get "https://localhost:8443/openapi/v2?timeout=32s": dial tcp 127.0.0.1:8443: connect: connection refused; if you choose to ignore these errors, turn validation off with --validate=false
I0513 11:29:09.174608    1464 ssh_runner.go:195] Run: sudo pgrep -xnf kube-apiserver.*minikube.*
I0513 11:29:09.235044    1464 api_server.go:72] duration metric: took 1.373030914s to wait for apiserver process to appear ...
I0513 11:29:09.235060    1464 api_server.go:88] waiting for apiserver healthz status ...
I0513 11:29:09.235080    1464 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0513 11:29:09.289789    1464 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml
I0513 11:29:09.428476    1464 ssh_runner.go:195] Run: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml
I0513 11:29:10.991399    1464 api_server.go:279] https://127.0.0.1:32769/healthz returned 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
W0513 11:29:10.991412    1464 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 403:
{"kind":"Status","apiVersion":"v1","metadata":{},"status":"Failure","message":"forbidden: User \"system:anonymous\" cannot get path \"/healthz\"","reason":"Forbidden","details":{},"code":403}
I0513 11:29:10.991423    1464 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0513 11:29:11.135577    1464 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0513 11:29:11.135595    1464 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[-]poststarthook/start-apiextensions-controllers failed: reason withheld
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[-]poststarthook/priority-and-fairness-config-producer failed: reason withheld
[+]poststarthook/start-system-namespaces-controller ok
[-]poststarthook/bootstrap-controller failed: reason withheld
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[-]poststarthook/apiservice-registration-controller failed: reason withheld
[+]poststarthook/apiservice-status-available-controller ok
[-]poststarthook/apiservice-discovery-controller failed: reason withheld
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0513 11:29:11.236243    1464 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0513 11:29:11.250290    1464 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0513 11:29:11.250313    1464 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0513 11:29:11.583293    1464 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storage-provisioner.yaml: (2.293483412s)
I0513 11:29:11.583432    1464 ssh_runner.go:235] Completed: sudo KUBECONFIG=/var/lib/minikube/kubeconfig /var/lib/minikube/binaries/v1.30.0/kubectl apply --force -f /etc/kubernetes/addons/storageclass.yaml: (2.154903992s)
I0513 11:29:11.597590    1464 out.go:177] 🌟  Enabled addons: storage-provisioner, default-storageclass
I0513 11:29:11.600407    1464 addons.go:505] duration metric: took 3.738313342s for enable addons: enabled=[storage-provisioner default-storageclass]
I0513 11:29:11.736475    1464 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0513 11:29:11.748511    1464 api_server.go:279] https://127.0.0.1:32769/healthz returned 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
W0513 11:29:11.748535    1464 api_server.go:103] status: https://127.0.0.1:32769/healthz returned error 500:
[+]ping ok
[+]log ok
[+]etcd ok
[+]poststarthook/start-apiserver-admission-initializer ok
[+]poststarthook/generic-apiserver-start-informers ok
[+]poststarthook/priority-and-fairness-config-consumer ok
[+]poststarthook/priority-and-fairness-filter ok
[+]poststarthook/storage-object-count-tracker-hook ok
[+]poststarthook/start-apiextensions-informers ok
[+]poststarthook/start-apiextensions-controllers ok
[+]poststarthook/crd-informer-synced ok
[+]poststarthook/start-service-ip-repair-controllers ok
[-]poststarthook/rbac/bootstrap-roles failed: reason withheld
[-]poststarthook/scheduling/bootstrap-system-priority-classes failed: reason withheld
[+]poststarthook/priority-and-fairness-config-producer ok
[+]poststarthook/start-system-namespaces-controller ok
[+]poststarthook/bootstrap-controller ok
[+]poststarthook/start-cluster-authentication-info-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-controller ok
[+]poststarthook/start-kube-apiserver-identity-lease-garbage-collector ok
[+]poststarthook/start-legacy-token-tracking-controller ok
[+]poststarthook/aggregator-reload-proxy-client-cert ok
[+]poststarthook/start-kube-aggregator-informers ok
[+]poststarthook/apiservice-registration-controller ok
[+]poststarthook/apiservice-status-available-controller ok
[+]poststarthook/apiservice-discovery-controller ok
[+]poststarthook/kube-apiserver-autoregistration ok
[+]autoregister-completion ok
[+]poststarthook/apiservice-openapi-controller ok
[+]poststarthook/apiservice-openapiv3-controller ok
healthz check failed
I0513 11:29:12.236186    1464 api_server.go:253] Checking apiserver healthz at https://127.0.0.1:32769/healthz ...
I0513 11:29:12.239347    1464 api_server.go:279] https://127.0.0.1:32769/healthz returned 200:
ok
I0513 11:29:12.240558    1464 api_server.go:141] control plane version: v1.30.0
I0513 11:29:12.240571    1464 api_server.go:131] duration metric: took 3.005505791s to wait for apiserver health ...
I0513 11:29:12.240578    1464 system_pods.go:43] waiting for kube-system pods to appear ...
I0513 11:29:12.245730    1464 system_pods.go:59] 7 kube-system pods found
I0513 11:29:12.245742    1464 system_pods.go:61] "coredns-7db6d8ff4d-54jmn" [5e9fb1cc-4f77-4b2d-821e-422987ed90f7] Running / Ready:ContainersNotReady (containers with unready status: [coredns]) / ContainersReady:ContainersNotReady (containers with unready status: [coredns])
I0513 11:29:12.245746    1464 system_pods.go:61] "etcd-minikube" [34fa3005-0833-4a91-a5ca-c40edf342cdb] Running / Ready:ContainersNotReady (containers with unready status: [etcd]) / ContainersReady:ContainersNotReady (containers with unready status: [etcd])
I0513 11:29:12.245750    1464 system_pods.go:61] "kube-apiserver-minikube" [df0c4042-bff3-4fe2-b06a-3e019f5ed58a] Running / Ready:ContainersNotReady (containers with unready status: [kube-apiserver]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-apiserver])
I0513 11:29:12.245753    1464 system_pods.go:61] "kube-controller-manager-minikube" [ada5e08a-0f3b-4c57-9179-aaf6f48051df] Running / Ready:ContainersNotReady (containers with unready status: [kube-controller-manager]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-controller-manager])
I0513 11:29:12.245756    1464 system_pods.go:61] "kube-proxy-qzsvj" [7a4d9e38-f15c-4bb3-91db-7accb32323d4] Running / Ready:ContainersNotReady (containers with unready status: [kube-proxy]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-proxy])
I0513 11:29:12.245758    1464 system_pods.go:61] "kube-scheduler-minikube" [f5697272-38b2-44b2-b728-b4e873e2568f] Running / Ready:ContainersNotReady (containers with unready status: [kube-scheduler]) / ContainersReady:ContainersNotReady (containers with unready status: [kube-scheduler])
I0513 11:29:12.245761    1464 system_pods.go:61] "storage-provisioner" [039e883d-b025-4014-9b85-57a1937efbc7] Running / Ready:ContainersNotReady (containers with unready status: [storage-provisioner]) / ContainersReady:ContainersNotReady (containers with unready status: [storage-provisioner])
I0513 11:29:12.245765    1464 system_pods.go:74] duration metric: took 5.183803ms to wait for pod list to return data ...
I0513 11:29:12.245771    1464 kubeadm.go:576] duration metric: took 4.383763708s to wait for: map[apiserver:true system_pods:true]
I0513 11:29:12.245777    1464 node_conditions.go:102] verifying NodePressure condition ...
I0513 11:29:12.249540    1464 node_conditions.go:122] node storage ephemeral capacity is 1055762868Ki
I0513 11:29:12.249549    1464 node_conditions.go:123] node cpu capacity is 12
I0513 11:29:12.249560    1464 node_conditions.go:105] duration metric: took 3.780601ms to run NodePressure ...
I0513 11:29:12.249568    1464 start.go:240] waiting for startup goroutines ...
I0513 11:29:12.249572    1464 start.go:245] waiting for cluster config update ...
I0513 11:29:12.249593    1464 start.go:254] writing updated cluster config ...
I0513 11:29:12.249721    1464 ssh_runner.go:195] Run: rm -f paused
I0513 11:29:12.564866    1464 start.go:600] kubectl: 1.30.0, cluster: 1.30.0 (minor skew: 0)
I0513 11:29:12.570835    1464 out.go:177] 🏄  Done! kubectl is now configured to use "minikube" cluster and "default" namespace by default


==> Docker <==
May 13 15:29:03 minikube systemd[1]: Stopped Docker Application Container Engine.
May 13 15:29:03 minikube systemd[1]: Starting Docker Application Container Engine...
May 13 15:29:03 minikube dockerd[792]: time="2024-05-13T15:29:03.453391133Z" level=info msg="Starting up"
May 13 15:29:03 minikube dockerd[792]: time="2024-05-13T15:29:03.470224210Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 13 15:29:03 minikube dockerd[792]: time="2024-05-13T15:29:03.486559784Z" level=info msg="Loading containers: start."
May 13 15:29:03 minikube dockerd[792]: time="2024-05-13T15:29:03.786353748Z" level=info msg="Processing signal 'terminated'"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.690916764Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.868459772Z" level=info msg="Loading containers: done."
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.889848669Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.889886769Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.889894569Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.889898269Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.889912469Z" level=info msg="Docker daemon" commit=60b9add containerd-snapshotter=false storage-driver=overlay2 version=26.0.1
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.889959769Z" level=info msg="Daemon has completed initialization"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.937877687Z" level=info msg="API listen on /var/run/docker.sock"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.937912188Z" level=info msg="API listen on [::]:2376"
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.939241394Z" level=info msg="stopping event stream following graceful shutdown" error="<nil>" module=libcontainerd namespace=moby
May 13 15:29:04 minikube dockerd[792]: time="2024-05-13T15:29:04.939980997Z" level=info msg="Daemon shutdown complete"
May 13 15:29:04 minikube systemd[1]: docker.service: Deactivated successfully.
May 13 15:29:04 minikube systemd[1]: Stopped Docker Application Container Engine.
May 13 15:29:04 minikube systemd[1]: Starting Docker Application Container Engine...
May 13 15:29:04 minikube dockerd[1033]: time="2024-05-13T15:29:04.984035797Z" level=info msg="Starting up"
May 13 15:29:05 minikube dockerd[1033]: time="2024-05-13T15:29:05.004611391Z" level=info msg="[graphdriver] trying configured driver: overlay2"
May 13 15:29:05 minikube dockerd[1033]: time="2024-05-13T15:29:05.018010252Z" level=info msg="Loading containers: start."
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.167565082Z" level=info msg="Default bridge (docker0) is assigned with an IP address 172.17.0.0/16. Daemon option --bip can be used to set a preferred IP address"
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.396662535Z" level=info msg="Loading containers: done."
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.408757436Z" level=warning msg="WARNING: No blkio throttle.read_bps_device support"
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.408791436Z" level=warning msg="WARNING: No blkio throttle.write_bps_device support"
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.408799036Z" level=warning msg="WARNING: No blkio throttle.read_iops_device support"
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.408802736Z" level=warning msg="WARNING: No blkio throttle.write_iops_device support"
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.408858937Z" level=info msg="Docker daemon" commit=60b9add containerd-snapshotter=false storage-driver=overlay2 version=26.0.1
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.408909737Z" level=info msg="Daemon has completed initialization"
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.459201044Z" level=info msg="API listen on /var/run/docker.sock"
May 13 15:29:06 minikube dockerd[1033]: time="2024-05-13T15:29:06.459282944Z" level=info msg="API listen on [::]:2376"
May 13 15:29:06 minikube systemd[1]: Started Docker Application Container Engine.
May 13 15:29:06 minikube systemd[1]: Starting CRI Interface for Docker Application Container Engine...
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Starting cri-dockerd dev (HEAD)"
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Connecting to docker on the Endpoint unix:///var/run/docker.sock"
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Start docker client with request timeout 0s"
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Hairpin mode is set to hairpin-veth"
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Loaded network plugin cni"
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Docker cri networking managed by network plugin cni"
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Setting cgroupDriver cgroupfs"
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:,},}"
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Starting the GRPC backend for the Docker CRI interface."
May 13 15:29:06 minikube cri-dockerd[1286]: time="2024-05-13T15:29:06Z" level=info msg="Start cri-dockerd grpc backend"
May 13 15:29:06 minikube systemd[1]: Started CRI Interface for Docker Application Container Engine.
May 13 15:29:07 minikube cri-dockerd[1286]: time="2024-05-13T15:29:07Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"thor-d964bcf5-g5bnk_default\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"18492e042dfd9e1032a4397c54f50aa17d68a5e04170dacaf0a599370371b4db\""
May 13 15:29:07 minikube cri-dockerd[1286]: time="2024-05-13T15:29:07Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-54jmn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1304330fdcab7a44a895324b4b591892283c77202b69fefe4ce511930dcbf82a\""
May 13 15:29:07 minikube cri-dockerd[1286]: time="2024-05-13T15:29:07Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-54jmn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"9ec238b18123216e6f13eaa0e44ffe129a200da35083bf438761f346309a8a49\""
May 13 15:29:08 minikube cri-dockerd[1286]: time="2024-05-13T15:29:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/9b34d1ad1ced51d90cad33faa2411f4e9c322306618770604580d3a44f1f35b7/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 13 15:29:08 minikube cri-dockerd[1286]: time="2024-05-13T15:29:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c791d6797749933ca07f559bef7b8ee37fc24ab2fef70b7b86f0d7471ad8211f/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 13 15:29:08 minikube cri-dockerd[1286]: time="2024-05-13T15:29:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/b5243df529b49a037177c9e8f49ed17ecd2d064389323ba9234cf232c77a33ff/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 13 15:29:08 minikube cri-dockerd[1286]: time="2024-05-13T15:29:08Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5c48dd11b21bf498f98dca3a507ede24a5347e6ced86135b515e863a5ebd66a7/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 13 15:29:09 minikube cri-dockerd[1286]: time="2024-05-13T15:29:09Z" level=info msg="Failed to read pod IP from plugin/docker: networkPlugin cni failed on the status hook for pod \"coredns-7db6d8ff4d-54jmn_kube-system\": CNI failed to retrieve network namespace path: cannot find network namespace for the terminated container \"1304330fdcab7a44a895324b4b591892283c77202b69fefe4ce511930dcbf82a\""
May 13 15:29:11 minikube cri-dockerd[1286]: time="2024-05-13T15:29:11Z" level=info msg="Docker cri received runtime config &RuntimeConfig{NetworkConfig:&NetworkConfig{PodCidr:10.244.0.0/24,},}"
May 13 15:29:12 minikube cri-dockerd[1286]: time="2024-05-13T15:29:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/5139d65137a7955d2b4bdac3e4e0313c42a3a52ade53b1327d779715eb0d88e2/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 13 15:29:12 minikube cri-dockerd[1286]: time="2024-05-13T15:29:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/ceaac2a66df77a16ae15885fffbcd354ba15c7d547e0316428f9631604fd1c30/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 13 15:29:12 minikube cri-dockerd[1286]: time="2024-05-13T15:29:12Z" level=info msg="Will attempt to re-write config file /var/lib/docker/containers/c4c1debc0ffd004fde21b40941b0243ab1a4e3edb45b59d0323826a7f61dd5ca/resolv.conf as [nameserver 192.168.49.1 options ndots:0]"
May 13 15:29:42 minikube dockerd[1033]: time="2024-05-13T15:29:42.638043146Z" level=info msg="ignoring event" container=20e27a7ef07a18fb108ed0293503049a365a852b5e63468b2502298a523103c5 module=libcontainerd namespace=moby topic=/tasks/delete type="*events.TaskDelete"


==> container status <==
CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID              POD
78a5000e48ba5       cbb01a7bd410d       33 seconds ago      Running             coredns                   3                   c4c1debc0ffd0       coredns-7db6d8ff4d-54jmn
4f4c94722ad2e       a0bf559e280cf       33 seconds ago      Running             kube-proxy                3                   ceaac2a66df77       kube-proxy-qzsvj
20e27a7ef07a1       6e38f40d628db       33 seconds ago      Exited              storage-provisioner       6                   5139d65137a79       storage-provisioner
a4e21d2547179       3861cfcd7c04c       37 seconds ago      Running             etcd                      3                   5c48dd11b21bf       etcd-minikube
597dbc827d45f       259c8277fcbbc       37 seconds ago      Running             kube-scheduler            3                   b5243df529b49       kube-scheduler-minikube
903cec8fcd478       c7aad43836fa5       37 seconds ago      Running             kube-controller-manager   3                   c791d67977499       kube-controller-manager-minikube
45a3a7636cb1c       c42f13656d0b2       37 seconds ago      Running             kube-apiserver            3                   9b34d1ad1ced5       kube-apiserver-minikube
94bee0d809c7a       cbb01a7bd410d       19 hours ago        Exited              coredns                   2                   1304330fdcab7       coredns-7db6d8ff4d-54jmn
7c9f7f92ddb28       a0bf559e280cf       19 hours ago        Exited              kube-proxy                2                   33b782a90f573       kube-proxy-qzsvj
5a0490d86d0fc       c7aad43836fa5       19 hours ago        Exited              kube-controller-manager   2                   f0e00b55dac16       kube-controller-manager-minikube
db0ec83643b10       3861cfcd7c04c       19 hours ago        Exited              etcd                      2                   5e0b2b9ec58b1       etcd-minikube
04a74edd66874       c42f13656d0b2       19 hours ago        Exited              kube-apiserver            2                   6837ea9db0178       kube-apiserver-minikube
f22eea241efa5       259c8277fcbbc       19 hours ago        Exited              kube-scheduler            2                   dbf9d3f1dc125       kube-scheduler-minikube


==> coredns [78a5000e48ba] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] 127.0.0.1:56040 - 60109 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 6.002540989s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:57777->192.168.49.1:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:36258 - 13744 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 6.002378729s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:36199->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:43000 - 28069 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 4.001652533s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:54826->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:52330 - 17383 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 2.001218084s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:54643->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:49260 - 21223 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 2.000984284s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:58181->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:38701 - 5734 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 2.000651997s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:53463->192.168.49.1:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:59929 - 1590 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 2.001593725s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:59025->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:53046 - 822 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 2.000748326s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:43289->192.168.49.1:53: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[352388352]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-May-2024 15:29:12.717) (total time: 30003ms):
Trace[352388352]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30001ms (15:29:42.719)
Trace[352388352]: [30.00304541s] [30.00304541s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1597447594]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-May-2024 15:29:12.717) (total time: 30003ms):
Trace[1597447594]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (15:29:42.719)
Trace[1597447594]: [30.00310481s] [30.00310481s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[551647741]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (13-May-2024 15:29:12.717) (total time: 30003ms):
Trace[551647741]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30002ms (15:29:42.719)
Trace[551647741]: [30.00318321s] [30.00318321s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] 127.0.0.1:52548 - 40345 "HINFO IN 7059902081598624604.8303980202233678824. udp 57 false 512" - - 0 2.001465436s
[ERROR] plugin/errors: 2 7059902081598624604.8303980202233678824. HINFO: read udp 10.244.0.7:53039->192.168.49.1:53: i/o timeout


==> coredns [94bee0d809c7] <==
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[INFO] plugin/kubernetes: waiting for Kubernetes API before starting server
[WARNING] plugin/kubernetes: starting server with unsynced Kubernetes API
.:53
[INFO] plugin/reload: Running configuration SHA512 = 05e3eaddc414b2d71a69b2e2bc6f2681fc1f4d04bcdd3acc1a41457bb7db518208b95ddfc4c9fffedc59c25a8faf458be1af4915a4a3c0d6777cb7a346bc5d86
CoreDNS-1.11.1
linux/amd64, go1.20.7, ae2bbc2
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:60698 - 36474 "HINFO IN 502052986048202684.5352098197293208626. udp 56 false 512" - - 0 6.001443294s
[ERROR] plugin/errors: 2 502052986048202684.5352098197293208626. HINFO: read udp 10.244.0.5:60216->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:34179 - 20464 "HINFO IN 502052986048202684.5352098197293208626. udp 56 false 512" - - 0 6.002545232s
[ERROR] plugin/errors: 2 502052986048202684.5352098197293208626. HINFO: read udp 10.244.0.5:33053->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:51439 - 10725 "HINFO IN 502052986048202684.5352098197293208626. udp 56 false 512" - - 0 4.001373155s
[ERROR] plugin/errors: 2 502052986048202684.5352098197293208626. HINFO: read udp 10.244.0.5:51882->192.168.49.1:53: i/o timeout
[INFO] 127.0.0.1:45211 - 61391 "HINFO IN 502052986048202684.5352098197293208626. udp 56 false 512" - - 0 2.000535178s
[ERROR] plugin/errors: 2 502052986048202684.5352098197293208626. HINFO: read udp 10.244.0.5:53953->192.168.49.1:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] 127.0.0.1:38791 - 28655 "HINFO IN 502052986048202684.5352098197293208626. udp 56 false 512" - - 0 2.000520841s
[ERROR] plugin/errors: 2 502052986048202684.5352098197293208626. HINFO: read udp 10.244.0.5:42490->192.168.49.1:53: i/o timeout
[INFO] plugin/ready: Still waiting on: "kubernetes"
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[957283532]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (12-May-2024 20:09:33.756) (total time: 30000ms):
Trace[957283532]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (20:10:03.757)
Trace[957283532]: [30.000844091s] [30.000844091s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Namespace: failed to list *v1.Namespace: Get "https://10.96.0.1:443/api/v1/namespaces?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[2061381735]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (12-May-2024 20:09:33.756) (total time: 30000ms):
Trace[2061381735]: ---"Objects listed" error:Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (20:10:03.757)
Trace[2061381735]: [30.000871991s] [30.000871991s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.EndpointSlice: failed to list *v1.EndpointSlice: Get "https://10.96.0.1:443/apis/discovery.k8s.io/v1/endpointslices?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] plugin/kubernetes: Trace[1402449951]: "Reflector ListAndWatch" name:pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231 (12-May-2024 20:09:33.756) (total time: 30001ms):
Trace[1402449951]: ---"Objects listed" error:Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout 30000ms (20:10:03.757)
Trace[1402449951]: [30.001016091s] [30.001016091s] END
[ERROR] plugin/kubernetes: pkg/mod/k8s.io/client-go@v0.27.4/tools/cache/reflector.go:231: Failed to watch *v1.Service: failed to list *v1.Service: Get "https://10.96.0.1:443/api/v1/services?limit=500&resourceVersion=0": dial tcp 10.96.0.1:443: i/o timeout
[INFO] SIGTERM: Shutting down servers then terminating
[INFO] plugin/health: Going into lameduck mode for 5s


==> describe nodes <==
Name:               minikube
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=minikube
                    kubernetes.io/os=linux
                    minikube.k8s.io/commit=86fc9d54fca63f295d8737c8eacdbb7987e89c67
                    minikube.k8s.io/name=minikube
                    minikube.k8s.io/primary=true
                    minikube.k8s.io/updated_at=2024_05_11T16_39_43_0700
                    minikube.k8s.io/version=v1.33.0
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Sat, 11 May 2024 20:39:40 +0000
Taints:             <none>
Unschedulable:      false
Lease:
  HolderIdentity:  minikube
  AcquireTime:     <unset>
  RenewTime:       Mon, 13 May 2024 15:29:41 +0000
Conditions:
  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message
  ----             ------  -----------------                 ------------------                ------                       -------
  MemoryPressure   False   Mon, 13 May 2024 15:29:11 +0000   Sat, 11 May 2024 20:39:38 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available
  DiskPressure     False   Mon, 13 May 2024 15:29:11 +0000   Sat, 11 May 2024 20:39:38 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure
  PIDPressure      False   Mon, 13 May 2024 15:29:11 +0000   Sat, 11 May 2024 20:39:38 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available
  Ready            True    Mon, 13 May 2024 15:29:11 +0000   Sat, 11 May 2024 20:39:40 +0000   KubeletReady                 kubelet is posting ready status
Addresses:
  InternalIP:  192.168.49.2
  Hostname:    minikube
Capacity:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16233716Ki
  pods:               110
Allocatable:
  cpu:                12
  ephemeral-storage:  1055762868Ki
  hugepages-1Gi:      0
  hugepages-2Mi:      0
  memory:             16233716Ki
  pods:               110
System Info:
  Machine ID:                 ec549c228f004d04adbea50b47071e7d
  System UUID:                ec549c228f004d04adbea50b47071e7d
  Boot ID:                    edcd7085-a1c0-402f-a02a-ab2ee861b3c3
  Kernel Version:             5.15.146.1-microsoft-standard-WSL2
  OS Image:                   Ubuntu 22.04.4 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  docker://26.0.1
  Kubelet Version:            v1.30.0
  Kube-Proxy Version:         v1.30.0
PodCIDR:                      10.244.0.0/24
PodCIDRs:                     10.244.0.0/24
Non-terminated Pods:          (7 in total)
  Namespace                   Name                                CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                ------------  ----------  ---------------  -------------  ---
  kube-system                 coredns-7db6d8ff4d-54jmn            100m (0%!)(MISSING)     0 (0%!)(MISSING)      70Mi (0%!)(MISSING)        170Mi (1%!)(MISSING)     42h
  kube-system                 etcd-minikube                       100m (0%!)(MISSING)     0 (0%!)(MISSING)      100Mi (0%!)(MISSING)       0 (0%!)(MISSING)         42h
  kube-system                 kube-apiserver-minikube             250m (2%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         42h
  kube-system                 kube-controller-manager-minikube    200m (1%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         42h
  kube-system                 kube-proxy-qzsvj                    0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         42h
  kube-system                 kube-scheduler-minikube             100m (0%!)(MISSING)     0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         42h
  kube-system                 storage-provisioner                 0 (0%!)(MISSING)        0 (0%!)(MISSING)      0 (0%!)(MISSING)           0 (0%!)(MISSING)         42h
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                750m (6%!)(MISSING)   0 (0%!)(MISSING)
  memory             170Mi (1%!)(MISSING)  170Mi (1%!)(MISSING)
  ephemeral-storage  0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-1Gi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
  hugepages-2Mi      0 (0%!)(MISSING)      0 (0%!)(MISSING)
Events:
  Type    Reason                   Age                From             Message
  ----    ------                   ----               ----             -------
  Normal  Starting                 42h                kube-proxy       
  Normal  Starting                 33s                kube-proxy       
  Normal  Starting                 19h                kube-proxy       
  Normal  Starting                 19h                kube-proxy       
  Normal  NodeHasNoDiskPressure    42h                kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeAllocatableEnforced  42h                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  42h                kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasSufficientPID     42h                kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 42h                kubelet          Starting kubelet.
  Normal  RegisteredNode           42h                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 19h                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  19h (x8 over 19h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    19h (x8 over 19h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     19h (x7 over 19h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  19h                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           19h                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  NodeAllocatableEnforced  19h                kubelet          Updated Node Allocatable limit across pods
  Normal  NodeHasSufficientMemory  19h (x8 over 19h)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    19h (x8 over 19h)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     19h (x7 over 19h)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  Starting                 19h                kubelet          Starting kubelet.
  Normal  RegisteredNode           19h                node-controller  Node minikube event: Registered Node minikube in Controller
  Normal  Starting                 38s                kubelet          Starting kubelet.
  Normal  NodeHasSufficientMemory  38s (x8 over 38s)  kubelet          Node minikube status is now: NodeHasSufficientMemory
  Normal  NodeHasNoDiskPressure    38s (x8 over 38s)  kubelet          Node minikube status is now: NodeHasNoDiskPressure
  Normal  NodeHasSufficientPID     38s (x7 over 38s)  kubelet          Node minikube status is now: NodeHasSufficientPID
  Normal  NodeAllocatableEnforced  38s                kubelet          Updated Node Allocatable limit across pods
  Normal  RegisteredNode           22s                node-controller  Node minikube event: Registered Node minikube in Controller


==> dmesg <==
[May13 15:25] MMIO Stale Data CPU bug present and SMT on, data leak possible. See https://www.kernel.org/doc/html/latest/admin-guide/hw-vuln/processor_mmio_stale_data.html for more details.
[  +0.000000]   #2  #3  #4  #5  #6  #7  #8  #9 #10 #11
[  +0.014392] PCI: Fatal: No config space access function found
[  +0.025725] PCI: System does not support PCI
[  +0.112032] kvm: already loaded the other module
[  +1.040013] FS-Cache: Duplicate cookie detected
[  +0.000541] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000809] FS-Cache: O-cookie d=00000000eab9ac64{9P.session} n=0000000097117b8b
[  +0.000658] FS-Cache: O-key=[10] '34323934393337343136'
[  +0.000358] FS-Cache: N-cookie c=00000005 [p=00000002 fl=2 nc=0 na=1]
[  +0.000688] FS-Cache: N-cookie d=00000000eab9ac64{9P.session} n=000000003b65a863
[  +0.000484] FS-Cache: N-key=[10] '34323934393337343136'
[  +0.001708] FS-Cache: Duplicate cookie detected
[  +0.000811] FS-Cache: O-cookie c=00000004 [p=00000002 fl=222 nc=0 na=1]
[  +0.000493] FS-Cache: O-cookie d=00000000eab9ac64{9P.session} n=0000000097117b8b
[  +0.000477] FS-Cache: O-key=[10] '34323934393337343136'
[  +0.000358] FS-Cache: N-cookie c=00000006 [p=00000002 fl=2 nc=0 na=1]
[  +0.000422] FS-Cache: N-cookie d=00000000eab9ac64{9P.session} n=000000002b62fc51
[  +0.000481] FS-Cache: N-key=[10] '34323934393337343136'
[  +0.486564] /sbin/ldconfig: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.196134] FS-Cache: Duplicate cookie detected
[  +0.000480] FS-Cache: O-cookie c=00000008 [p=00000002 fl=222 nc=0 na=1]
[  +0.000427] FS-Cache: O-cookie d=00000000eab9ac64{9P.session} n=0000000089913e2b
[  +0.000479] FS-Cache: O-key=[10] '34323934393337343835'
[  +0.000306] FS-Cache: N-cookie c=00000009 [p=00000002 fl=2 nc=0 na=1]
[  +0.000506] FS-Cache: N-cookie d=00000000eab9ac64{9P.session} n=000000002f67a5f8
[  +0.000677] FS-Cache: N-key=[10] '34323934393337343835'
[  +0.091766] /sbin/ldconfig.real: 
[  +0.000004] /usr/lib/wsl/lib/libcuda.so.1 is not a symbolic link

[  +0.046559] FS-Cache: Duplicate cookie detected
[  +0.000534] FS-Cache: O-cookie c=0000000b [p=00000002 fl=222 nc=0 na=1]
[  +0.000394] FS-Cache: O-cookie d=00000000eab9ac64{9P.session} n=00000000c2993c15
[  +0.000487] FS-Cache: O-key=[10] '34323934393337343939'
[  +0.000319] FS-Cache: N-cookie c=0000000c [p=00000002 fl=2 nc=0 na=1]
[  +0.000391] FS-Cache: N-cookie d=00000000eab9ac64{9P.session} n=000000004e5175b3
[  +0.000472] FS-Cache: N-key=[10] '34323934393337343939'
[  +0.542102] systemd-journald[61]: File /var/log/journal/57d2e4d59803a5c2c1cc14726501e47d/system.journal corrupted or uncleanly shut down, renaming and replacing.
[  +1.511900] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000583] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000424] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000516] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +0.000847] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000423] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000418] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -22
[  +0.000522] misc dxg: dxgk: dxgkio_query_adapter_info: Ioctl failed: -2
[  +8.387889] WSL (2) ERROR: WaitForBootProcess:3335: /sbin/init failed to start within 10000
[  +0.000005] ms
[May13 15:26] systemd-journald[61]: File /var/log/journal/57d2e4d59803a5c2c1cc14726501e47d/user-1000.journal corrupted or uncleanly shut down, renaming and replacing.


==> etcd [a4e21d254717] <==
{"level":"warn","ts":"2024-05-13T15:29:08.977016Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-13T15:29:08.977903Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-05-13T15:29:08.977959Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-05-13T15:29:08.977996Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-13T15:29:08.978007Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-13T15:29:08.978062Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-13T15:29:08.981926Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-05-13T15:29:08.982593Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-05-13T15:29:08.996308Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"13.288602ms"}
{"level":"info","ts":"2024-05-13T15:29:09.00918Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-05-13T15:29:09.01876Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":3103}
{"level":"info","ts":"2024-05-13T15:29:09.01946Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-05-13T15:29:09.019499Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 4"}
{"level":"info","ts":"2024-05-13T15:29:09.01951Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 4, commit: 3103, applied: 0, lastindex: 3103, lastterm: 4]"}
{"level":"warn","ts":"2024-05-13T15:29:09.024219Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-05-13T15:29:09.026058Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1548}
{"level":"info","ts":"2024-05-13T15:29:09.037674Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":2565}
{"level":"info","ts":"2024-05-13T15:29:09.040872Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-05-13T15:29:09.044264Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-05-13T15:29:09.044809Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-13T15:29:09.044869Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-05-13T15:29:09.045093Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-05-13T15:29:09.0456Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-05-13T15:29:09.045714Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-13T15:29:09.045826Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-13T15:29:09.045872Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-13T15:29:09.046089Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-13T15:29:09.046208Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-13T15:29:09.046223Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-13T15:29:09.048716Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-13T15:29:09.048873Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-13T15:29:09.048944Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-13T15:29:09.048952Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-05-13T15:29:09.048976Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-05-13T15:29:10.019876Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 4"}
{"level":"info","ts":"2024-05-13T15:29:10.019924Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 4"}
{"level":"info","ts":"2024-05-13T15:29:10.019949Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-05-13T15:29:10.01996Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 5"}
{"level":"info","ts":"2024-05-13T15:29:10.019964Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-05-13T15:29:10.019971Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 5"}
{"level":"info","ts":"2024-05-13T15:29:10.019976Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 5"}
{"level":"info","ts":"2024-05-13T15:29:10.023Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-13T15:29:10.02305Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-13T15:29:10.023144Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-13T15:29:10.023363Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-13T15:29:10.02339Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-13T15:29:10.025472Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-05-13T15:29:10.025484Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}


==> etcd [db0ec83643b1] <==
{"level":"warn","ts":"2024-05-12T20:09:30.003521Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-12T20:09:30.003619Z","caller":"etcdmain/etcd.go:73","msg":"Running: ","args":["etcd","--advertise-client-urls=https://192.168.49.2:2379","--cert-file=/var/lib/minikube/certs/etcd/server.crt","--client-cert-auth=true","--data-dir=/var/lib/minikube/etcd","--experimental-initial-corrupt-check=true","--experimental-watch-progress-notify-interval=5s","--initial-advertise-peer-urls=https://192.168.49.2:2380","--initial-cluster=minikube=https://192.168.49.2:2380","--key-file=/var/lib/minikube/certs/etcd/server.key","--listen-client-urls=https://127.0.0.1:2379,https://192.168.49.2:2379","--listen-metrics-urls=http://127.0.0.1:2381","--listen-peer-urls=https://192.168.49.2:2380","--name=minikube","--peer-cert-file=/var/lib/minikube/certs/etcd/peer.crt","--peer-client-cert-auth=true","--peer-key-file=/var/lib/minikube/certs/etcd/peer.key","--peer-trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt","--proxy-refresh-interval=70000","--snapshot-count=10000","--trusted-ca-file=/var/lib/minikube/certs/etcd/ca.crt"]}
{"level":"info","ts":"2024-05-12T20:09:30.003671Z","caller":"etcdmain/etcd.go:116","msg":"server has been already initialized","data-dir":"/var/lib/minikube/etcd","dir-type":"member"}
{"level":"warn","ts":"2024-05-12T20:09:30.003691Z","caller":"embed/config.go:679","msg":"Running http and grpc server on single port. This is not recommended for production."}
{"level":"info","ts":"2024-05-12T20:09:30.003696Z","caller":"embed/etcd.go:127","msg":"configuring peer listeners","listen-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-12T20:09:30.003742Z","caller":"embed/etcd.go:494","msg":"starting with peer TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/peer.crt, key = /var/lib/minikube/certs/etcd/peer.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-12T20:09:30.004402Z","caller":"embed/etcd.go:135","msg":"configuring client listeners","listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"]}
{"level":"info","ts":"2024-05-12T20:09:30.004512Z","caller":"embed/etcd.go:308","msg":"starting an etcd server","etcd-version":"3.5.12","git-sha":"e7b3bb6cc","go-version":"go1.20.13","go-os":"linux","go-arch":"amd64","max-cpu-set":12,"max-cpu-available":12,"member-initialized":true,"name":"minikube","data-dir":"/var/lib/minikube/etcd","wal-dir":"","wal-dir-dedicated":"","member-dir":"/var/lib/minikube/etcd/member","force-new-cluster":false,"heartbeat-interval":"100ms","election-timeout":"1s","initial-election-tick-advance":true,"snapshot-count":10000,"max-wals":5,"max-snapshots":5,"snapshot-catchup-entries":5000,"initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"],"cors":["*"],"host-whitelist":["*"],"initial-cluster":"","initial-cluster-state":"new","initial-cluster-token":"","quota-backend-bytes":2147483648,"max-request-bytes":1572864,"max-concurrent-streams":4294967295,"pre-vote":true,"initial-corrupt-check":true,"corrupt-check-time-interval":"0s","compact-check-time-enabled":false,"compact-check-time-interval":"1m0s","auto-compaction-mode":"periodic","auto-compaction-retention":"0s","auto-compaction-interval":"0s","discovery-url":"","discovery-proxy":"","downgrade-check-interval":"5s"}
{"level":"info","ts":"2024-05-12T20:09:30.005735Z","caller":"etcdserver/backend.go:81","msg":"opened backend db","path":"/var/lib/minikube/etcd/member/snap/db","took":"1.0635ms"}
{"level":"info","ts":"2024-05-12T20:09:30.015108Z","caller":"etcdserver/server.go:532","msg":"No snapshot found. Recovering WAL from scratch!"}
{"level":"info","ts":"2024-05-12T20:09:30.021284Z","caller":"etcdserver/raft.go:530","msg":"restarting local member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","commit-index":2927}
{"level":"info","ts":"2024-05-12T20:09:30.021534Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=()"}
{"level":"info","ts":"2024-05-12T20:09:30.021576Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became follower at term 3"}
{"level":"info","ts":"2024-05-12T20:09:30.021586Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"newRaft aec36adc501070cc [peers: [], term: 3, commit: 2927, applied: 0, lastindex: 2927, lastterm: 3]"}
{"level":"warn","ts":"2024-05-12T20:09:30.023726Z","caller":"auth/store.go:1241","msg":"simple token is not cryptographically signed"}
{"level":"info","ts":"2024-05-12T20:09:30.025568Z","caller":"mvcc/kvstore.go:341","msg":"restored last compact revision","meta-bucket-name":"meta","meta-bucket-name-key":"finishedCompactRev","restored-compact-revision":1548}
{"level":"info","ts":"2024-05-12T20:09:30.027718Z","caller":"mvcc/kvstore.go:407","msg":"kvstore restored","current-rev":2410}
{"level":"info","ts":"2024-05-12T20:09:30.031032Z","caller":"etcdserver/quota.go:94","msg":"enabled backend quota with default value","quota-name":"v3-applier","quota-size-bytes":2147483648,"quota-size":"2.1 GB"}
{"level":"info","ts":"2024-05-12T20:09:30.034411Z","caller":"etcdserver/corrupt.go:96","msg":"starting initial corruption check","local-member-id":"aec36adc501070cc","timeout":"7s"}
{"level":"info","ts":"2024-05-12T20:09:30.034691Z","caller":"etcdserver/corrupt.go:177","msg":"initial corruption checking passed; no corruption","local-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-12T20:09:30.034728Z","caller":"etcdserver/server.go:860","msg":"starting etcd server","local-member-id":"aec36adc501070cc","local-server-version":"3.5.12","cluster-version":"to_be_decided"}
{"level":"info","ts":"2024-05-12T20:09:30.036419Z","caller":"embed/etcd.go:726","msg":"starting with client TLS","tls-info":"cert = /var/lib/minikube/certs/etcd/server.crt, key = /var/lib/minikube/certs/etcd/server.key, client-cert=, client-key=, trusted-ca = /var/lib/minikube/certs/etcd/ca.crt, client-cert-auth = true, crl-file = ","cipher-suites":[]}
{"level":"info","ts":"2024-05-12T20:09:30.036561Z","caller":"embed/etcd.go:277","msg":"now serving peer/client/metrics","local-member-id":"aec36adc501070cc","initial-advertise-peer-urls":["https://192.168.49.2:2380"],"listen-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"],"listen-client-urls":["https://127.0.0.1:2379","https://192.168.49.2:2379"],"listen-metrics-urls":["http://127.0.0.1:2381"]}
{"level":"info","ts":"2024-05-12T20:09:30.036597Z","caller":"embed/etcd.go:857","msg":"serving metrics","address":"http://127.0.0.1:2381"}
{"level":"info","ts":"2024-05-12T20:09:30.036668Z","caller":"etcdserver/server.go:760","msg":"starting initial election tick advance","election-ticks":10}
{"level":"info","ts":"2024-05-12T20:09:30.036778Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap.db","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-12T20:09:30.036816Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/snap","suffix":"snap","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-12T20:09:30.036824Z","caller":"fileutil/purge.go:50","msg":"started to purge file","dir":"/var/lib/minikube/etcd/member/wal","suffix":"wal","max":5,"interval":"30s"}
{"level":"info","ts":"2024-05-12T20:09:30.097703Z","caller":"embed/etcd.go:597","msg":"serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-12T20:09:30.098056Z","caller":"embed/etcd.go:569","msg":"cmux::serve","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-12T20:09:30.097867Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc switched to configuration voters=(12593026477526642892)"}
{"level":"info","ts":"2024-05-12T20:09:30.098285Z","caller":"membership/cluster.go:421","msg":"added member","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","added-peer-id":"aec36adc501070cc","added-peer-peer-urls":["https://192.168.49.2:2380"]}
{"level":"info","ts":"2024-05-12T20:09:30.098465Z","caller":"membership/cluster.go:584","msg":"set initial cluster version","cluster-id":"fa54960ea34d58be","local-member-id":"aec36adc501070cc","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-12T20:09:30.098514Z","caller":"api/capability.go:75","msg":"enabled capabilities for version","cluster-version":"3.5"}
{"level":"info","ts":"2024-05-12T20:09:31.322091Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc is starting a new election at term 3"}
{"level":"info","ts":"2024-05-12T20:09:31.322293Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became pre-candidate at term 3"}
{"level":"info","ts":"2024-05-12T20:09:31.322354Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgPreVoteResp from aec36adc501070cc at term 3"}
{"level":"info","ts":"2024-05-12T20:09:31.32239Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became candidate at term 4"}
{"level":"info","ts":"2024-05-12T20:09:31.32241Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc received MsgVoteResp from aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-05-12T20:09:31.322435Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"aec36adc501070cc became leader at term 4"}
{"level":"info","ts":"2024-05-12T20:09:31.322532Z","logger":"raft","caller":"etcdserver/zap_raft.go:77","msg":"raft.node: aec36adc501070cc elected leader aec36adc501070cc at term 4"}
{"level":"info","ts":"2024-05-12T20:09:31.334385Z","caller":"etcdserver/server.go:2068","msg":"published local member to cluster through raft","local-member-id":"aec36adc501070cc","local-member-attributes":"{Name:minikube ClientURLs:[https://192.168.49.2:2379]}","request-path":"/0/members/aec36adc501070cc/attributes","cluster-id":"fa54960ea34d58be","publish-timeout":"7s"}
{"level":"info","ts":"2024-05-12T20:09:31.334533Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-12T20:09:31.334829Z","caller":"embed/serve.go:103","msg":"ready to serve client requests"}
{"level":"info","ts":"2024-05-12T20:09:31.335655Z","caller":"etcdmain/main.go:44","msg":"notifying init daemon"}
{"level":"info","ts":"2024-05-12T20:09:31.335805Z","caller":"etcdmain/main.go:50","msg":"successfully notified init daemon"}
{"level":"info","ts":"2024-05-12T20:09:31.339046Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"127.0.0.1:2379"}
{"level":"info","ts":"2024-05-12T20:09:31.339087Z","caller":"embed/serve.go:250","msg":"serving client traffic securely","traffic":"grpc+http","address":"192.168.49.2:2379"}
{"level":"info","ts":"2024-05-12T20:10:43.335278Z","caller":"osutil/interrupt_unix.go:64","msg":"received signal; shutting down","signal":"terminated"}
{"level":"info","ts":"2024-05-12T20:10:43.335331Z","caller":"embed/etcd.go:375","msg":"closing etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}
{"level":"warn","ts":"2024-05-12T20:10:43.335395Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-12T20:10:43.335472Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 127.0.0.1:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-12T20:10:43.402253Z","caller":"embed/serve.go:212","msg":"stopping secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"warn","ts":"2024-05-12T20:10:43.402311Z","caller":"embed/serve.go:214","msg":"stopped secure grpc server due to error","error":"accept tcp 192.168.49.2:2379: use of closed network connection"}
{"level":"info","ts":"2024-05-12T20:10:43.403765Z","caller":"etcdserver/server.go:1471","msg":"skipped leadership transfer for single voting member cluster","local-member-id":"aec36adc501070cc","current-leader-member-id":"aec36adc501070cc"}
{"level":"info","ts":"2024-05-12T20:10:43.410108Z","caller":"embed/etcd.go:579","msg":"stopping serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-12T20:10:43.410323Z","caller":"embed/etcd.go:584","msg":"stopped serving peer traffic","address":"192.168.49.2:2380"}
{"level":"info","ts":"2024-05-12T20:10:43.410346Z","caller":"embed/etcd.go:377","msg":"closed etcd server","name":"minikube","data-dir":"/var/lib/minikube/etcd","advertise-peer-urls":["https://192.168.49.2:2380"],"advertise-client-urls":["https://192.168.49.2:2379"]}


==> kernel <==
 15:29:45 up 4 min,  0 users,  load average: 0.38, 0.23, 0.09
Linux minikube 5.15.146.1-microsoft-standard-WSL2 #1 SMP Thu Jan 11 04:09:03 UTC 2024 x86_64 x86_64 x86_64 GNU/Linux
PRETTY_NAME="Ubuntu 22.04.4 LTS"


==> kube-apiserver [04a74edd6687] <==
W0512 20:10:48.974888       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:49.043454       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:49.075493       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:49.182632       1 logging.go:59] [core] [Channel #82 SubChannel #83] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.315723       1 logging.go:59] [core] [Channel #100 SubChannel #101] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.406181       1 logging.go:59] [core] [Channel #106 SubChannel #107] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.419374       1 logging.go:59] [core] [Channel #2 SubChannel #3] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.437590       1 logging.go:59] [core] [Channel #166 SubChannel #167] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.442347       1 logging.go:59] [core] [Channel #139 SubChannel #140] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.518339       1 logging.go:59] [core] [Channel #127 SubChannel #128] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.556948       1 logging.go:59] [core] [Channel #70 SubChannel #71] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.744303       1 logging.go:59] [core] [Channel #103 SubChannel #104] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.847237       1 logging.go:59] [core] [Channel #46 SubChannel #47] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.852049       1 logging.go:59] [core] [Channel #112 SubChannel #113] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.888912       1 logging.go:59] [core] [Channel #151 SubChannel #152] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.895483       1 logging.go:59] [core] [Channel #148 SubChannel #149] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.930702       1 logging.go:59] [core] [Channel #52 SubChannel #53] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.949704       1 logging.go:59] [core] [Channel #25 SubChannel #26] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.949807       1 logging.go:59] [core] [Channel #133 SubChannel #134] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.968573       1 logging.go:59] [core] [Channel #160 SubChannel #161] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:51.992324       1 logging.go:59] [core] [Channel #169 SubChannel #170] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.056554       1 logging.go:59] [core] [Channel #34 SubChannel #35] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.132293       1 logging.go:59] [core] [Channel #67 SubChannel #68] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.142643       1 logging.go:59] [core] [Channel #73 SubChannel #74] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.154870       1 logging.go:59] [core] [Channel #154 SubChannel #155] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.196376       1 logging.go:59] [core] [Channel #19 SubChannel #20] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.281945       1 logging.go:59] [core] [Channel #175 SubChannel #176] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.300453       1 logging.go:59] [core] [Channel #118 SubChannel #119] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.320160       1 logging.go:59] [core] [Channel #76 SubChannel #77] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.340093       1 logging.go:59] [core] [Channel #178 SubChannel #179] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.365407       1 logging.go:59] [core] [Channel #157 SubChannel #158] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.393635       1 logging.go:59] [core] [Channel #43 SubChannel #44] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.408985       1 logging.go:59] [core] [Channel #64 SubChannel #65] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.442740       1 logging.go:59] [core] [Channel #97 SubChannel #98] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.447294       1 logging.go:59] [core] [Channel #61 SubChannel #62] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.453998       1 logging.go:59] [core] [Channel #163 SubChannel #164] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.508018       1 logging.go:59] [core] [Channel #109 SubChannel #110] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.570421       1 logging.go:59] [core] [Channel #145 SubChannel #146] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.640232       1 logging.go:59] [core] [Channel #88 SubChannel #89] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.665760       1 logging.go:59] [core] [Channel #49 SubChannel #50] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.686370       1 logging.go:59] [core] [Channel #85 SubChannel #86] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.691359       1 logging.go:59] [core] [Channel #181 SubChannel #182] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.692338       1 logging.go:59] [core] [Channel #79 SubChannel #80] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.694665       1 logging.go:59] [core] [Channel #55 SubChannel #56] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.719361       1 logging.go:59] [core] [Channel #130 SubChannel #131] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.726234       1 logging.go:59] [core] [Channel #94 SubChannel #95] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.859656       1 logging.go:59] [core] [Channel #28 SubChannel #29] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.873972       1 logging.go:59] [core] [Channel #37 SubChannel #38] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.885961       1 logging.go:59] [core] [Channel #22 SubChannel #23] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.908802       1 logging.go:59] [core] [Channel #115 SubChannel #116] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.967805       1 logging.go:59] [core] [Channel #15 SubChannel #16] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:52.967811       1 logging.go:59] [core] [Channel #10 SubChannel #11] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:53.104365       1 logging.go:59] [core] [Channel #91 SubChannel #92] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:53.129654       1 logging.go:59] [core] [Channel #124 SubChannel #125] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:53.185767       1 logging.go:59] [core] [Channel #31 SubChannel #32] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:53.216535       1 logging.go:59] [core] [Channel #136 SubChannel #137] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:53.272280       1 logging.go:59] [core] [Channel #142 SubChannel #143] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:53.286042       1 logging.go:59] [core] [Channel #5 SubChannel #6] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:53.295059       1 logging.go:59] [core] [Channel #121 SubChannel #122] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"
W0512 20:10:53.348229       1 logging.go:59] [core] [Channel #172 SubChannel #173] grpc: addrConn.createTransport failed to connect to {Addr: "127.0.0.1:2379", ServerName: "127.0.0.1:2379", }. Err: connection error: desc = "transport: Error while dialing: dial tcp 127.0.0.1:2379: connect: connection refused"


==> kube-apiserver [45a3a7636cb1] <==
I0513 15:29:10.670568       1 handler.go:286] Adding GroupVersion admissionregistration.k8s.io v1 to ResourceManager
W0513 15:29:10.670596       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1beta1 because it has no resources.
W0513 15:29:10.670601       1 genericapiserver.go:733] Skipping API admissionregistration.k8s.io/v1alpha1 because it has no resources.
I0513 15:29:10.671070       1 handler.go:286] Adding GroupVersion events.k8s.io v1 to ResourceManager
W0513 15:29:10.671094       1 genericapiserver.go:733] Skipping API events.k8s.io/v1beta1 because it has no resources.
I0513 15:29:10.679374       1 handler.go:286] Adding GroupVersion apiregistration.k8s.io v1 to ResourceManager
W0513 15:29:10.679410       1 genericapiserver.go:733] Skipping API apiregistration.k8s.io/v1beta1 because it has no resources.
I0513 15:29:10.981786       1 secure_serving.go:213] Serving securely on [::]:8443
I0513 15:29:10.982126       1 available_controller.go:423] Starting AvailableConditionController
I0513 15:29:10.982138       1 cache.go:32] Waiting for caches to sync for AvailableConditionController controller
I0513 15:29:10.982166       1 controller.go:78] Starting OpenAPI AggregationController
I0513 15:29:10.982203       1 apiservice_controller.go:97] Starting APIServiceRegistrationController
I0513 15:29:10.982229       1 cache.go:32] Waiting for caches to sync for APIServiceRegistrationController controller
I0513 15:29:10.982230       1 controller.go:116] Starting legacy_token_tracking_controller
I0513 15:29:10.982262       1 shared_informer.go:313] Waiting for caches to sync for configmaps
I0513 15:29:10.982223       1 system_namespaces_controller.go:67] Starting system namespaces controller
I0513 15:29:10.983965       1 dynamic_serving_content.go:132] "Starting controller" name="serving-cert::/var/lib/minikube/certs/apiserver.crt::/var/lib/minikube/certs/apiserver.key"
I0513 15:29:10.984021       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0513 15:29:10.984000       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0513 15:29:10.983991       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0513 15:29:10.984236       1 gc_controller.go:78] Starting apiserver lease garbage collector
I0513 15:29:10.984303       1 customresource_discovery_controller.go:289] Starting DiscoveryController
I0513 15:29:10.984425       1 controller.go:139] Starting OpenAPI controller
I0513 15:29:10.984525       1 controller.go:87] Starting OpenAPI V3 controller
I0513 15:29:10.984618       1 naming_controller.go:291] Starting NamingConditionController
I0513 15:29:10.984640       1 establishing_controller.go:76] Starting EstablishingController
I0513 15:29:10.984649       1 crd_finalizer.go:266] Starting CRDFinalizer
I0513 15:29:10.984653       1 apiapproval_controller.go:186] Starting KubernetesAPIApprovalPolicyConformantConditionController
I0513 15:29:10.984669       1 nonstructuralschema_controller.go:192] Starting NonStructuralSchemaConditionController
I0513 15:29:10.984982       1 aggregator.go:163] waiting for initial CRD sync...
I0513 15:29:10.985039       1 apf_controller.go:374] Starting API Priority and Fairness config controller
I0513 15:29:10.985110       1 cluster_authentication_trust_controller.go:440] Starting cluster_authentication_trust_controller controller
I0513 15:29:10.985126       1 shared_informer.go:313] Waiting for caches to sync for cluster_authentication_trust_controller
I0513 15:29:10.985148       1 dynamic_serving_content.go:132] "Starting controller" name="aggregator-proxy-cert::/var/lib/minikube/certs/front-proxy-client.crt::/var/lib/minikube/certs/front-proxy-client.key"
I0513 15:29:10.985172       1 dynamic_cafile_content.go:157] "Starting controller" name="client-ca-bundle::/var/lib/minikube/certs/ca.crt"
I0513 15:29:10.985212       1 crdregistration_controller.go:111] Starting crd-autoregister controller
I0513 15:29:10.985221       1 shared_informer.go:313] Waiting for caches to sync for crd-autoregister
I0513 15:29:10.985213       1 dynamic_cafile_content.go:157] "Starting controller" name="request-header::/var/lib/minikube/certs/front-proxy-ca.crt"
I0513 15:29:10.985242       1 controller.go:80] Starting OpenAPI V3 AggregationController
I0513 15:29:11.052426       1 shared_informer.go:320] Caches are synced for node_authorizer
I0513 15:29:11.057764       1 shared_informer.go:320] Caches are synced for *generic.policySource[*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicy,*k8s.io/api/admissionregistration/v1.ValidatingAdmissionPolicyBinding,k8s.io/apiserver/pkg/admission/plugin/policy/validating.Validator]
I0513 15:29:11.134561       1 policy_source.go:224] refreshing policies
I0513 15:29:11.134680       1 shared_informer.go:320] Caches are synced for configmaps
I0513 15:29:11.134694       1 shared_informer.go:320] Caches are synced for cluster_authentication_trust_controller
I0513 15:29:11.134728       1 cache.go:39] Caches are synced for AvailableConditionController controller
I0513 15:29:11.134809       1 shared_informer.go:320] Caches are synced for crd-autoregister
I0513 15:29:11.134878       1 apf_controller.go:379] Running API Priority and Fairness config worker
I0513 15:29:11.134889       1 apf_controller.go:382] Running API Priority and Fairness periodic rebalancing process
I0513 15:29:11.135272       1 aggregator.go:165] initial CRD sync complete...
I0513 15:29:11.135306       1 autoregister_controller.go:141] Starting autoregister controller
I0513 15:29:11.135313       1 cache.go:32] Waiting for caches to sync for autoregister controller
I0513 15:29:11.135318       1 cache.go:39] Caches are synced for autoregister controller
I0513 15:29:11.135494       1 cache.go:39] Caches are synced for APIServiceRegistrationController controller
I0513 15:29:11.138759       1 controller.go:615] quota admission added evaluator for: leases.coordination.k8s.io
I0513 15:29:11.140006       1 handler_discovery.go:447] Starting ResourceDiscoveryManager
E0513 15:29:11.147281       1 controller.go:97] Error removing old endpoints from kubernetes service: no API server IP addresses were listed in storage, refusing to erase all endpoints for the kubernetes Service
E0513 15:29:11.153163       1 controller.go:195] "Failed to update lease" err="Operation cannot be fulfilled on leases.coordination.k8s.io \"apiserver-eqt674mfxb4j56mrjjkoe7b7ii\": StorageError: invalid object, Code: 4, Key: /registry/leases/kube-system/apiserver-eqt674mfxb4j56mrjjkoe7b7ii, ResourceVersion: 0, AdditionalErrorMsg: Precondition failed: UID in precondition: 1fea6503-85f4-48dc-b1a8-bb1fc606097c, UID in object meta: "
I0513 15:29:11.987425       1 storage_scheduling.go:111] all system priority classes are created successfully or already exist.
I0513 15:29:24.058216       1 controller.go:615] quota admission added evaluator for: endpointslices.discovery.k8s.io
I0513 15:29:24.098669       1 controller.go:615] quota admission added evaluator for: endpoints


==> kube-controller-manager [5a0490d86d0f] <==
I0512 20:09:45.305272       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0512 20:09:45.305297       1 shared_informer.go:320] Caches are synced for endpoint
I0512 20:09:45.305426       1 shared_informer.go:320] Caches are synced for expand
I0512 20:09:45.305454       1 shared_informer.go:320] Caches are synced for stateful set
I0512 20:09:45.305469       1 shared_informer.go:320] Caches are synced for cronjob
I0512 20:09:45.306700       1 shared_informer.go:320] Caches are synced for crt configmap
I0512 20:09:45.310105       1 shared_informer.go:320] Caches are synced for HPA
I0512 20:09:45.311157       1 shared_informer.go:320] Caches are synced for deployment
I0512 20:09:45.313590       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0512 20:09:45.317341       1 shared_informer.go:320] Caches are synced for TTL after finished
I0512 20:09:45.322188       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0512 20:09:45.322920       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0512 20:09:45.322925       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0512 20:09:45.322982       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0512 20:09:45.323275       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0512 20:09:45.324985       1 shared_informer.go:320] Caches are synced for TTL
I0512 20:09:45.327331       1 shared_informer.go:320] Caches are synced for ephemeral
I0512 20:09:45.328538       1 shared_informer.go:320] Caches are synced for GC
I0512 20:09:45.347295       1 shared_informer.go:320] Caches are synced for job
I0512 20:09:45.349572       1 shared_informer.go:320] Caches are synced for disruption
I0512 20:09:45.352935       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0512 20:09:45.354191       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0512 20:09:45.354643       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0512 20:09:45.354710       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0512 20:09:45.354901       1 shared_informer.go:320] Caches are synced for ReplicationController
I0512 20:09:45.356113       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0512 20:09:45.356188       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0512 20:09:45.366341       1 shared_informer.go:320] Caches are synced for node
I0512 20:09:45.366437       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0512 20:09:45.366455       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0512 20:09:45.366484       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0512 20:09:45.366491       1 shared_informer.go:320] Caches are synced for cidrallocator
I0512 20:09:45.369618       1 shared_informer.go:320] Caches are synced for PVC protection
I0512 20:09:45.370247       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="15.5034ms"
I0512 20:09:45.370341       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="38.9µs"
I0512 20:09:45.408662       1 shared_informer.go:320] Caches are synced for daemon sets
I0512 20:09:45.410992       1 shared_informer.go:320] Caches are synced for taint
I0512 20:09:45.411067       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0512 20:09:45.411122       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0512 20:09:45.411214       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0512 20:09:45.415821       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0512 20:09:45.445952       1 shared_informer.go:320] Caches are synced for namespace
I0512 20:09:45.455079       1 shared_informer.go:320] Caches are synced for service account
I0512 20:09:45.514967       1 shared_informer.go:320] Caches are synced for persistent volume
I0512 20:09:45.526511       1 shared_informer.go:320] Caches are synced for attach detach
I0512 20:09:45.537963       1 shared_informer.go:320] Caches are synced for resource quota
I0512 20:09:45.559200       1 shared_informer.go:320] Caches are synced for PV protection
I0512 20:09:45.575812       1 shared_informer.go:320] Caches are synced for resource quota
I0512 20:09:45.985462       1 shared_informer.go:320] Caches are synced for garbage collector
I0512 20:09:46.021612       1 shared_informer.go:320] Caches are synced for garbage collector
I0512 20:09:46.021645       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0512 20:09:53.321925       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thor-d964bcf5" duration="17.928099ms"
I0512 20:09:53.326141       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thor-d964bcf5" duration="4.155ms"
I0512 20:09:53.326250       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thor-d964bcf5" duration="36.1µs"
I0512 20:09:53.329629       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thor-d964bcf5" duration="56.5µs"
I0512 20:10:09.925893       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thor-d964bcf5" duration="25.2µs"
I0512 20:10:11.895161       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="7.7334ms"
I0512 20:10:11.895247       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="37.5µs"
I0512 20:10:20.837288       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thor-d964bcf5" duration="36.9µs"
I0512 20:10:36.085081       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="default/thor-d964bcf5" duration="5.1µs"


==> kube-controller-manager [903cec8fcd47] <==
I0513 15:29:23.346351       1 shared_informer.go:320] Caches are synced for token_cleaner
I0513 15:29:23.401875       1 controllermanager.go:759] "Started controller" controller="persistentvolume-attach-detach-controller"
I0513 15:29:23.401910       1 controllermanager.go:737] "Warning: skipping controller" controller="storage-version-migrator-controller"
I0513 15:29:23.402077       1 attach_detach_controller.go:342] "Starting attach detach controller" logger="persistentvolume-attach-detach-controller"
I0513 15:29:23.402128       1 shared_informer.go:313] Waiting for caches to sync for attach detach
I0513 15:29:23.407640       1 shared_informer.go:313] Waiting for caches to sync for resource quota
I0513 15:29:23.414293       1 shared_informer.go:320] Caches are synced for bootstrap_signer
I0513 15:29:23.414682       1 actual_state_of_world.go:543] "Failed to update statusUpdateNeeded field in actual state of world" logger="persistentvolume-attach-detach-controller" err="Failed to set statusUpdateNeeded to needed true, because nodeName=\"minikube\" does not exist"
I0513 15:29:23.415674       1 shared_informer.go:313] Waiting for caches to sync for garbage collector
I0513 15:29:23.418300       1 shared_informer.go:320] Caches are synced for ClusterRoleAggregator
I0513 15:29:23.426182       1 shared_informer.go:320] Caches are synced for namespace
I0513 15:29:23.427460       1 shared_informer.go:320] Caches are synced for certificate-csrapproving
I0513 15:29:23.431105       1 shared_informer.go:320] Caches are synced for crt configmap
I0513 15:29:23.458611       1 shared_informer.go:320] Caches are synced for service account
I0513 15:29:23.458680       1 shared_informer.go:320] Caches are synced for TTL
I0513 15:29:23.461092       1 shared_informer.go:320] Caches are synced for endpoint_slice_mirroring
I0513 15:29:23.470565       1 shared_informer.go:320] Caches are synced for cronjob
I0513 15:29:23.473076       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-serving
I0513 15:29:23.473147       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kube-apiserver-client
I0513 15:29:23.473243       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-kubelet-client
I0513 15:29:23.473277       1 shared_informer.go:320] Caches are synced for certificate-csrsigning-legacy-unknown
I0513 15:29:23.476537       1 shared_informer.go:320] Caches are synced for TTL after finished
I0513 15:29:23.480012       1 shared_informer.go:320] Caches are synced for PV protection
I0513 15:29:23.504749       1 shared_informer.go:320] Caches are synced for node
I0513 15:29:23.504802       1 range_allocator.go:175] "Sending events to api server" logger="node-ipam-controller"
I0513 15:29:23.504815       1 range_allocator.go:179] "Starting range CIDR allocator" logger="node-ipam-controller"
I0513 15:29:23.504833       1 shared_informer.go:313] Waiting for caches to sync for cidrallocator
I0513 15:29:23.504837       1 shared_informer.go:320] Caches are synced for cidrallocator
I0513 15:29:23.517809       1 shared_informer.go:320] Caches are synced for expand
I0513 15:29:23.546478       1 shared_informer.go:320] Caches are synced for validatingadmissionpolicy-status
I0513 15:29:23.660243       1 shared_informer.go:320] Caches are synced for taint-eviction-controller
I0513 15:29:23.662739       1 shared_informer.go:320] Caches are synced for stateful set
I0513 15:29:23.664964       1 shared_informer.go:320] Caches are synced for HPA
I0513 15:29:23.671303       1 shared_informer.go:320] Caches are synced for disruption
I0513 15:29:23.680543       1 shared_informer.go:320] Caches are synced for PVC protection
I0513 15:29:23.682844       1 shared_informer.go:320] Caches are synced for ReplicationController
I0513 15:29:23.684073       1 shared_informer.go:320] Caches are synced for daemon sets
I0513 15:29:23.685271       1 shared_informer.go:320] Caches are synced for ReplicaSet
I0513 15:29:23.696155       1 shared_informer.go:320] Caches are synced for deployment
I0513 15:29:23.697422       1 shared_informer.go:320] Caches are synced for legacy-service-account-token-cleaner
I0513 15:29:23.702774       1 shared_informer.go:320] Caches are synced for attach detach
I0513 15:29:23.705689       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="20.332101ms"
I0513 15:29:23.706287       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="60.9µs"
I0513 15:29:23.706795       1 shared_informer.go:320] Caches are synced for taint
I0513 15:29:23.706867       1 node_lifecycle_controller.go:1227] "Initializing eviction metric for zone" logger="node-lifecycle-controller" zone=""
I0513 15:29:23.706916       1 node_lifecycle_controller.go:879] "Missing timestamp for Node. Assuming now as a timestamp" logger="node-lifecycle-controller" node="minikube"
I0513 15:29:23.706978       1 node_lifecycle_controller.go:1073] "Controller detected that zone is now in new state" logger="node-lifecycle-controller" zone="" newState="Normal"
I0513 15:29:23.707892       1 shared_informer.go:320] Caches are synced for resource quota
I0513 15:29:23.712320       1 shared_informer.go:320] Caches are synced for job
I0513 15:29:23.716630       1 shared_informer.go:320] Caches are synced for GC
I0513 15:29:23.728784       1 shared_informer.go:320] Caches are synced for persistent volume
I0513 15:29:23.733121       1 shared_informer.go:320] Caches are synced for ephemeral
I0513 15:29:23.748207       1 shared_informer.go:320] Caches are synced for endpoint_slice
I0513 15:29:23.756906       1 shared_informer.go:320] Caches are synced for endpoint
I0513 15:29:23.757022       1 shared_informer.go:320] Caches are synced for resource quota
I0513 15:29:24.110485       1 shared_informer.go:320] Caches are synced for garbage collector
I0513 15:29:24.110512       1 garbagecollector.go:157] "All resource monitors have synced. Proceeding to collect garbage" logger="garbage-collector-controller"
I0513 15:29:24.116584       1 shared_informer.go:320] Caches are synced for garbage collector
I0513 15:29:45.352965       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="9.398204ms"
I0513 15:29:45.353071       1 replica_set.go:676] "Finished syncing" logger="replicaset-controller" kind="ReplicaSet" key="kube-system/coredns-7db6d8ff4d" duration="46.6µs"


==> kube-proxy [4f4c94722ad2] <==
I0513 15:29:12.701984       1 server_linux.go:69] "Using iptables proxy"
I0513 15:29:12.719036       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0513 15:29:12.743267       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0513 15:29:12.743311       1 server_linux.go:165] "Using iptables Proxier"
I0513 15:29:12.744725       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0513 15:29:12.744754       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0513 15:29:12.745730       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0513 15:29:12.746277       1 server.go:872] "Version info" version="v1.30.0"
I0513 15:29:12.746319       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0513 15:29:12.748094       1 config.go:192] "Starting service config controller"
I0513 15:29:12.748335       1 config.go:101] "Starting endpoint slice config controller"
I0513 15:29:12.748359       1 config.go:319] "Starting node config controller"
I0513 15:29:12.748452       1 shared_informer.go:313] Waiting for caches to sync for service config
I0513 15:29:12.748452       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0513 15:29:12.748468       1 shared_informer.go:313] Waiting for caches to sync for node config
I0513 15:29:12.848984       1 shared_informer.go:320] Caches are synced for service config
I0513 15:29:12.848975       1 shared_informer.go:320] Caches are synced for node config
I0513 15:29:12.849047       1 shared_informer.go:320] Caches are synced for endpoint slice config


==> kube-proxy [7c9f7f92ddb2] <==
I0512 20:09:33.654161       1 server_linux.go:69] "Using iptables proxy"
I0512 20:09:33.660180       1 server.go:1062] "Successfully retrieved node IP(s)" IPs=["192.168.49.2"]
I0512 20:09:33.706595       1 server.go:659] "kube-proxy running in dual-stack mode" primary ipFamily="IPv4"
I0512 20:09:33.706657       1 server_linux.go:165] "Using iptables Proxier"
I0512 20:09:33.708612       1 server_linux.go:511] "Detect-local-mode set to ClusterCIDR, but no cluster CIDR for family" ipFamily="IPv6"
I0512 20:09:33.708662       1 server_linux.go:528] "Defaulting to no-op detect-local"
I0512 20:09:33.708713       1 proxier.go:243] "Setting route_localnet=1 to allow node-ports on localhost; to change this either disable iptables.localhostNodePorts (--iptables-localhost-nodeports) or set nodePortAddresses (--nodeport-addresses) to filter loopback addresses"
I0512 20:09:33.708960       1 server.go:872] "Version info" version="v1.30.0"
I0512 20:09:33.708994       1 server.go:874] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0512 20:09:33.709956       1 config.go:192] "Starting service config controller"
I0512 20:09:33.709996       1 shared_informer.go:313] Waiting for caches to sync for service config
I0512 20:09:33.709973       1 config.go:101] "Starting endpoint slice config controller"
I0512 20:09:33.710014       1 shared_informer.go:313] Waiting for caches to sync for endpoint slice config
I0512 20:09:33.710052       1 config.go:319] "Starting node config controller"
I0512 20:09:33.710082       1 shared_informer.go:313] Waiting for caches to sync for node config
I0512 20:09:33.810671       1 shared_informer.go:320] Caches are synced for endpoint slice config
I0512 20:09:33.810767       1 shared_informer.go:320] Caches are synced for service config
I0512 20:09:33.810795       1 shared_informer.go:320] Caches are synced for node config


==> kube-scheduler [597dbc827d45] <==
I0513 15:29:09.592343       1 serving.go:380] Generated self-signed cert in-memory
I0513 15:29:11.141521       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0513 15:29:11.141567       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0513 15:29:11.153680       1 requestheader_controller.go:169] Starting RequestHeaderAuthRequestController
I0513 15:29:11.153721       1 shared_informer.go:313] Waiting for caches to sync for RequestHeaderAuthRequestController
I0513 15:29:11.153729       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0513 15:29:11.153733       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file"
I0513 15:29:11.153744       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0513 15:29:11.153751       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file
I0513 15:29:11.153841       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0513 15:29:11.153902       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0513 15:29:11.254508       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0513 15:29:11.254630       1 shared_informer.go:320] Caches are synced for RequestHeaderAuthRequestController
I0513 15:29:11.254777       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::requestheader-client-ca-file


==> kube-scheduler [f22eea241efa] <==
I0512 20:09:30.443264       1 serving.go:380] Generated self-signed cert in-memory
W0512 20:09:32.131583       1 requestheader_controller.go:193] Unable to get configmap/extension-apiserver-authentication in kube-system.  Usually fixed by 'kubectl create rolebinding -n kube-system ROLEBINDING_NAME --role=extension-apiserver-authentication-reader --serviceaccount=YOUR_NS:YOUR_SA'
W0512 20:09:32.131767       1 authentication.go:368] Error looking up in-cluster authentication configuration: configmaps "extension-apiserver-authentication" is forbidden: User "system:kube-scheduler" cannot get resource "configmaps" in API group "" in the namespace "kube-system"
W0512 20:09:32.131841       1 authentication.go:369] Continuing without authentication configuration. This may treat all requests as anonymous.
W0512 20:09:32.131905       1 authentication.go:370] To require authentication configuration lookup to succeed, set --authentication-tolerate-lookup-failure=false
I0512 20:09:32.210448       1 server.go:154] "Starting Kubernetes Scheduler" version="v1.30.0"
I0512 20:09:32.210500       1 server.go:156] "Golang settings" GOGC="" GOMAXPROCS="" GOTRACEBACK=""
I0512 20:09:32.212078       1 configmap_cafile_content.go:202] "Starting controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0512 20:09:32.212155       1 shared_informer.go:313] Waiting for caches to sync for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0512 20:09:32.212224       1 secure_serving.go:213] Serving securely on 127.0.0.1:10259
I0512 20:09:32.212400       1 tlsconfig.go:240] "Starting DynamicServingCertificateController"
I0512 20:09:32.312461       1 shared_informer.go:320] Caches are synced for client-ca::kube-system::extension-apiserver-authentication::client-ca-file
I0512 20:10:43.334148       1 tlsconfig.go:255] "Shutting down DynamicServingCertificateController"
I0512 20:10:43.334240       1 configmap_cafile_content.go:223] "Shutting down controller" name="client-ca::kube-system::extension-apiserver-authentication::client-ca-file"
I0512 20:10:43.334648       1 secure_serving.go:258] Stopped listening on 127.0.0.1:10259
E0512 20:10:43.334750       1 run.go:74] "command failed" err="finished without leader elect"


==> kubelet <==
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.074409    1499 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c"} err="rpc error: code = Unknown desc = failed to remove container \"cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c\": Error response from daemon: removal of container cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c is already in progress"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.074441    1499 scope.go:117] "RemoveContainer" containerID="cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.138309    1499 scope.go:117] "RemoveContainer" containerID="c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.140739    1499 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.141472    1499 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c\": Error response from daemon: No such container: cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c" containerID="cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.141715    1499 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c"} err="rpc error: code = Unknown desc = failed to remove container \"cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c\": Error response from daemon: No such container: cb93084d37bf95f593519065bf8f6ad0411a553ffe3894f998497f754558ce5c"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.141853    1499 scope.go:117] "RemoveContainer" containerID="c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.142073    1499 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.149077    1499 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="4ec2ac102781907c4943c44a8d096230c719cdd590bb062785ee447fadb7fd5f"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.149181    1499 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="6837ea9db01789417275967fbb7f26ad71fd3de60e38603c90562e1df84db82f"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.149207    1499 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="db9e4fd6bccd60b43a613313b2ec204c8a0aaa87e03fe131c78b1d6d9ccc33cf"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.165771    1499 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38\": Error response from daemon: removal of container c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38 is already in progress" containerID="c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.165842    1499 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38"} err="rpc error: code = Unknown desc = failed to remove container \"c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38\": Error response from daemon: removal of container c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38 is already in progress"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.165868    1499 scope.go:117] "RemoveContainer" containerID="c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.241370    1499 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38\": Error response from daemon: removal of container c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38 is already in progress" containerID="c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.241500    1499 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38"} err="rpc error: code = Unknown desc = failed to remove container \"c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38\": Error response from daemon: removal of container c68c9499ece3227b035d6772f3ab2714f0d60027a2d760b535d8b3c5cc77df38 is already in progress"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.241557    1499 scope.go:117] "RemoveContainer" containerID="f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.254226    1499 scope.go:117] "RemoveContainer" containerID="f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.267770    1499 remote_runtime.go:385] "RemoveContainer from runtime service failed" err="rpc error: code = Unknown desc = failed to remove container \"f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd\": Error response from daemon: removal of container f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd is already in progress" containerID="f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.267818    1499 kuberuntime_gc.go:150] "Failed to remove container" err="rpc error: code = Unknown desc = failed to remove container \"f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd\": Error response from daemon: removal of container f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd is already in progress" containerID="f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.283943    1499 scope.go:117] "RemoveContainer" containerID="f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.284812    1499 remote_runtime.go:432] "ContainerStatus from runtime service failed" err="rpc error: code = Unknown desc = Error response from daemon: No such container: f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd" containerID="f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.284854    1499 pod_container_deletor.go:53] "DeleteContainer returned error" containerID={"Type":"docker","ID":"f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd"} err="failed to get container status \"f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd\": rpc error: code = Unknown desc = Error response from daemon: No such container: f3c25a3ffbef2902b11261d25db93db62dfc81e3edfd7144627e01490864e7cd"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.437213    1499 controller.go:145] "Failed to ensure lease exists, will retry" err="Get \"https://control-plane.minikube.internal:8443/apis/coordination.k8s.io/v1/namespaces/kube-node-lease/leases/minikube?timeout=10s\": dial tcp 192.168.49.2:8443: connect: connection refused" interval="800ms"
May 13 15:29:08 minikube kubelet[1499]: I0513 15:29:08.543517    1499 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.543971    1499 kubelet_node_status.go:96] "Unable to register node with API server" err="Post \"https://control-plane.minikube.internal:8443/api/v1/nodes\": dial tcp 192.168.49.2:8443: connect: connection refused" node="minikube"
May 13 15:29:08 minikube kubelet[1499]: W0513 15:29:08.673202    1499 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.673262    1499 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.RuntimeClass: failed to list *v1.RuntimeClass: Get "https://control-plane.minikube.internal:8443/apis/node.k8s.io/v1/runtimeclasses?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
May 13 15:29:08 minikube kubelet[1499]: W0513 15:29:08.836947    1499 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
May 13 15:29:08 minikube kubelet[1499]: E0513 15:29:08.837252    1499 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.Node: failed to list *v1.Node: Get "https://control-plane.minikube.internal:8443/api/v1/nodes?fieldSelector=metadata.name%!D(MISSING)minikube&limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
May 13 15:29:09 minikube kubelet[1499]: W0513 15:29:09.136117    1499 reflector.go:547] k8s.io/client-go/informers/factory.go:160: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
May 13 15:29:09 minikube kubelet[1499]: E0513 15:29:09.136355    1499 reflector.go:150] k8s.io/client-go/informers/factory.go:160: Failed to watch *v1.CSIDriver: failed to list *v1.CSIDriver: Get "https://control-plane.minikube.internal:8443/apis/storage.k8s.io/v1/csidrivers?limit=500&resourceVersion=0": dial tcp 192.168.49.2:8443: connect: connection refused
May 13 15:29:09 minikube kubelet[1499]: I0513 15:29:09.346443    1499 kubelet_node_status.go:73] "Attempting to register node" node="minikube"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.155387    1499 kubelet_node_status.go:112] "Node was previously registered" node="minikube"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.155482    1499 kubelet_node_status.go:76] "Successfully registered node" node="minikube"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.156375    1499 kuberuntime_manager.go:1523] "Updating runtime config through cri with podcidr" CIDR="10.244.0.0/24"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.159580    1499 kubelet_network.go:61] "Updating Pod CIDR" originalPodCIDR="" newPodCIDR="10.244.0.0/24"
May 13 15:29:11 minikube kubelet[1499]: E0513 15:29:11.278899    1499 kubelet.go:1928] "Failed creating a mirror pod for" err="pods \"kube-apiserver-minikube\" already exists" pod="kube-system/kube-apiserver-minikube"
May 13 15:29:11 minikube kubelet[1499]: E0513 15:29:11.811824    1499 kubelet.go:1928] "Failed creating a mirror pod for" err="pods \"kube-controller-manager-minikube\" already exists" pod="kube-system/kube-controller-manager-minikube"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.836351    1499 apiserver.go:52] "Watching apiserver"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.840128    1499 topology_manager.go:215] "Topology Admit Handler" podUID="039e883d-b025-4014-9b85-57a1937efbc7" podNamespace="kube-system" podName="storage-provisioner"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.840227    1499 topology_manager.go:215] "Topology Admit Handler" podUID="5e9fb1cc-4f77-4b2d-821e-422987ed90f7" podNamespace="kube-system" podName="coredns-7db6d8ff4d-54jmn"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.840330    1499 topology_manager.go:215] "Topology Admit Handler" podUID="7a4d9e38-f15c-4bb3-91db-7accb32323d4" podNamespace="kube-system" podName="kube-proxy-qzsvj"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.919300    1499 desired_state_of_world_populator.go:157] "Finished populating initial desired state of world"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.946978    1499 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"tmp\" (UniqueName: \"kubernetes.io/host-path/039e883d-b025-4014-9b85-57a1937efbc7-tmp\") pod \"storage-provisioner\" (UID: \"039e883d-b025-4014-9b85-57a1937efbc7\") " pod="kube-system/storage-provisioner"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.947275    1499 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"xtables-lock\" (UniqueName: \"kubernetes.io/host-path/7a4d9e38-f15c-4bb3-91db-7accb32323d4-xtables-lock\") pod \"kube-proxy-qzsvj\" (UID: \"7a4d9e38-f15c-4bb3-91db-7accb32323d4\") " pod="kube-system/kube-proxy-qzsvj"
May 13 15:29:11 minikube kubelet[1499]: I0513 15:29:11.947381    1499 reconciler_common.go:247] "operationExecutor.VerifyControllerAttachedVolume started for volume \"lib-modules\" (UniqueName: \"kubernetes.io/host-path/7a4d9e38-f15c-4bb3-91db-7accb32323d4-lib-modules\") pod \"kube-proxy-qzsvj\" (UID: \"7a4d9e38-f15c-4bb3-91db-7accb32323d4\") " pod="kube-system/kube-proxy-qzsvj"
May 13 15:29:12 minikube kubelet[1499]: I0513 15:29:12.348790    1499 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="5139d65137a7955d2b4bdac3e4e0313c42a3a52ade53b1327d779715eb0d88e2"
May 13 15:29:12 minikube kubelet[1499]: I0513 15:29:12.416280    1499 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="c4c1debc0ffd004fde21b40941b0243ab1a4e3edb45b59d0323826a7f61dd5ca"
May 13 15:29:12 minikube kubelet[1499]: I0513 15:29:12.551642    1499 pod_container_deletor.go:80] "Container not found in pod's containers" containerID="ceaac2a66df77a16ae15885fffbcd354ba15c7d547e0316428f9631604fd1c30"
May 13 15:29:14 minikube kubelet[1499]: I0513 15:29:14.598118    1499 prober_manager.go:312] "Failed to trigger a manual run" probe="Readiness"
May 13 15:29:17 minikube kubelet[1499]: E0513 15:29:17.880800    1499 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
May 13 15:29:17 minikube kubelet[1499]: E0513 15:29:17.880845    1499 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
May 13 15:29:27 minikube kubelet[1499]: E0513 15:29:27.893637    1499 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
May 13 15:29:27 minikube kubelet[1499]: E0513 15:29:27.893673    1499 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
May 13 15:29:37 minikube kubelet[1499]: E0513 15:29:37.905878    1499 summary_sys_containers.go:48] "Failed to get system container stats" err="failed to get cgroup stats for \"/kubepods\": failed to get container info for \"/kubepods\": unknown container \"/kubepods\"" containerName="/kubepods"
May 13 15:29:37 minikube kubelet[1499]: E0513 15:29:37.905945    1499 helpers.go:857] "Eviction manager: failed to construct signal" err="system container \"pods\" not found in metrics" signal="allocatableMemory.available"
May 13 15:29:42 minikube kubelet[1499]: I0513 15:29:42.816362    1499 scope.go:117] "RemoveContainer" containerID="9eb6657387bfff3d61f3dc3a3c0899c911e0ddede26a7097aaa9f7abf2de2745"
May 13 15:29:42 minikube kubelet[1499]: I0513 15:29:42.816509    1499 scope.go:117] "RemoveContainer" containerID="20e27a7ef07a18fb108ed0293503049a365a852b5e63468b2502298a523103c5"
May 13 15:29:42 minikube kubelet[1499]: E0513 15:29:42.816717    1499 pod_workers.go:1298] "Error syncing pod, skipping" err="failed to \"StartContainer\" for \"storage-provisioner\" with CrashLoopBackOff: \"back-off 10s restarting failed container=storage-provisioner pod=storage-provisioner_kube-system(039e883d-b025-4014-9b85-57a1937efbc7)\"" pod="kube-system/storage-provisioner" podUID="039e883d-b025-4014-9b85-57a1937efbc7"


==> storage-provisioner [20e27a7ef07a] <==
I0513 15:29:12.621079       1 storage_provisioner.go:116] Initializing the minikube storage provisioner...
F0513 15:29:42.628301       1 main.go:39] error getting server version: Get "https://10.96.0.1:443/version?timeout=32s": dial tcp 10.96.0.1:443: i/o timeout

